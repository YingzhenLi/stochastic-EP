\section{Introduction}

Recently a number of methods have been developed for applying Bayesian learning to large datasets \todo[fancyline]{poss justify why Bayes+Big data (Jordan)}. Examples include sampling such as ... \cite{adams}, distributional approximations including stochastic variational inference \cite{hoffman:svi} and assumed density filtering \cite{}, and approaches that mix distributional and sampling approximations \cite{gelman,teh}. 

One family of approximation method has garnered less attention in this regard: Expectation propagation (EP) \cite{minka:ep}\cite{opper:ec} (see section \ref{sec:} for an introduction). EP constructs a posterior approximation by iterating simple local computations that refine factors which approximate the posterior-contribution from each datapoint. At first sight, it therefore appears well suited to large-data problems: the locality of computation make the algorithm simple to parallelise and distribute, and good practical performance on a range of small-data applications suggest that it will be accurate\cite{kuss:gpep,barthelme:ep_likelihood,cunningham}. On the other hand, there is a critical computational bottleneck in the standard EP approximation: each local approximating factor typically has the same complexity as the global approximation. This means that the EP algorithm has a memory overhead that grows with the number of data-points $N$, which makes it hard to scale rich models with many parameters to large-data settings (consider approximating the posterior distribution of a topic model with millions of parameters on a corpus containing billions of documents). The same pathology exists for the broader class of power-EP (PEP) algorithms \cite{minka:pep} that includes variational message-passing \cite{winn:vmp}. The elegance of local computation has been bought at the price of having myriad local approximating factors. In contrast, variational inference (VI) methods \cite{jordan:variational,beal:variational} utilise global approximations that are refined directly (rather than through local components) which prevents memory overheads from scaling with $N$. 

Is there ever a case for preferring EP (or PEP) to VI methods in large-data settings?  We believe that there certainly is. First, EP can provide significantly more accurate approximations for certain model classes. It is well known that variational free-energy approaches are biased and often severely so \cite{turner+sahani:2011} and for particular models the variational free-energy objective is pathologically ill-suited\cite{cunningham,turner}. Second, the fact that EP is truely local (to factors in the posterior distribution and not just likelihoods) means that it affords more opportunities for tractable algorithm design (the probabilistic back-propagation setting treated later in the paper is one such case where PEP is tractable, but variational free-energy methods require a second level of approximation\todo[fancyline]{double check with Miguel that this is correct}). 

As EP appears to be the method of choice for some applications, researchers have attempted to push it to scale. One approach is to swallow the large computational burden and simply use large data-structures to store the approximating factors (e.g.~TrueSkill \cite{herbrich:trueskill}) \todo[fancyline]{YL: Is this what you meant? YES by Yingzhen}. This approach can only be pushed so far. A second approach is to use a simple variant of EP called assumed density filtering (ADF) which only requires a global approximation to be stored \cite{ADF}. ADF, however, provides poorly calibrated uncertainty estimates \cite{minka:ep} which was one of the main motivating reasons for developing EP in the first place. Another idea uses EP to carve up the dataset \cite{gelman:dep,xu:sms} by using approximating factors for collections of data-points. This results in coarse-grained, rather than local, updates and sampling based methods must be used to compute them. (Indeed, the spirit of this work is to extend sampling methods to large-datasets, rather than EP itself.) 

The question addressed in this paper is: can we have the best of both worlds? That is, accurate global approximations that are derived from truely local computation. For this purpose we develop an algorithm based upon the standard EP and ADF algorithms that maintains a global approximation which is updated in a local way. We call this class of algorithms stochastic EP (SEP) since it updates the global approximation with (damped) stochastic estimates on data sub-samples in an analogous way to SVI. Indeed, the generalisation of the algorithm to the PEP setting directly relates to SVI \cite{hoffman:svi}. Tests on synthetic data indicate that SEP performs almost as well as full EP, but reduces the memory footprint by a factor of $N$, and that it has well calibrated uncertainty estimates, unlike ADF. We show how to extend the method to treat models with latent variables without compromising the accuracy or memory demands. Finally, we demonstrate the scalability and accuracy of the method on a number of classification and regression datasets.


%Bayesian inference is gaining attentions in machine learning. It requires the posterior distribution after seeing the observations, however that posterior is generally uncomputable due to the intractability of data likelihood and/or the growing size of the dataset. Approximate methods are developed for such computation, with sampling approaches and variational approximations as two main directions. The latter one provides deterministic approximations to the posterior distributions in considerably shorter computing time, where important examples include variational inference (VI)/variational Bayes (VB) \cite{jordan:variational}\cite{beal:variational}, and expectation propagation (EP) \cite{minka:ep}\cite{opper:ec}. Especially variational inference has attracted significant interests in modelling very large datasets, thanks to its fast computation with stochastic optimisation techniques \cite{hoffman:svi}. Seeing the success of VI/VB for big data, a question arises naturally: are other variational methods, including EP, also work well for large datasets?

%%% 
%Unfortunately, there exist far less practical studies showing EP's performance on very large datasets, despite its great success in a wide range of applications including Gaussian process classification \cite{kuss:gpep} and likelihood-free inference \cite{barthelme:ep_likelihood}. The main obstacle for EP's application to big data is its huge amount of storage requirement, even though the algorithm itself converges very fast in practice and is easy to parallelise. Compare with variational inference which stores the posterior approximation only, EP forms a collection of local approximations, each of the same space complexity as the global one, and memorises all of them for future updates. Typical number of local approximations is equal to the number of observations, making EP be less applicable for large datasets. Existing research did not tackle this problem directly: commercial systems (e.g.~TrueSkill \cite{herbrich:trueskill}) simply use very large databases, while data partitioning methods \cite{gelman:dep}\cite{xu:sms} propose local approximations on disjoint subsets but with the price of computation time for advanced sampling methods.

%In this work we introduce a stochastic approximation of EP, which computes nearly identical steps as EP but memorises the global poster approximation only. We name this method stochastic expectation propagation (SEP) as it updates the global approximation with (damped) stochastic estimates on data sub-samples. We derive the expectation version of this stochastic algorithm, show SEP's connections to stochastic variational inference \cite{hoffman:svi}, and discuss further applications in distributed settings. Synthetic tests indicate that SEP performs almost as good as full EP, and further real-world experiments also show the scalability of our proposed approach.