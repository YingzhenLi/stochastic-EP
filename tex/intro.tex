\section{Introduction}
Bayesian inference is gaining attentions in machine learning. It requires the posterior distribution after seeing the observations, however that posterior is generally uncomputable due to the intractability of data likelihood and/or the growing size of the dataset. Approximate methods are developed for such computation, with sampling approaches and variational approximations as two main directions. The latter one provides deterministic approximations to the posterior distributions in considerably shorter computing time, where important examples include variational inference (VI)/variational Bayes (VB) \cite{jordan:variational}\cite{beal:variational}, and expectation propagation (EP) \cite{minka:ep}\cite{opper:ec}. Especially variational inference has attracted significant interests in modelling very large datasets, thanks to its fast computation with stochastic optimisation techniques \cite{hoffman:svi}. Seeing the success of VI/VB for big data, a question arises naturally: are other variational methods, including EP, also work well for large datasets?

%%% 
Unfortunately, there exist far less practical studies showing EP's performance on very large datasets, despite its great success in a wide range of applications including Gaussian process classification \cite{kuss:gpep} and likelihood-free inference \cite{barthelme:ep_likelihood}. The main obstacle for EP's application to big data is its huge amount of storage requirement, even though the algorithm itself converges very fast in practice and is easy to parallelise. Compare with variational inference which stores the posterior approximation only, EP forms a collection of local approximations, each of the same space complexity as the global one, and memorises all of them for future updates. Typical number of local approximations is equal to the number of observations, making EP be less applicable for large datasets. Existing research did not tackle this problem directly: commercial systems (e.g.~TrueSkill \cite{herbrich:trueskill}) simply use very large databases, while data partitioning methods \cite{gelman:dep}\cite{xu:sms} propose local approximations on disjoint subsets but with the price of computation time for advanced sampling methods.

In this work we introduce a stochastic approximation of EP, which computes nearly identical steps as EP but memorises the global poster approximation only. We name this method stochastic expectation propagation (SEP) as it updates the global approximation with (damped) stochastic estimates on data sub-samples. We derive the expectation version of this stochastic algorithm, show SEP's connections to stochastic variational inference \cite{hoffman:svi}, and discuss further applications in distributed settings. Synthetic tests indicate that SEP performs almost as good as full EP, and further real-world experiments also show the scalability of our proposed approach.