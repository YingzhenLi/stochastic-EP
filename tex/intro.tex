\section{Introduction}


Recently a number of methods have been developed for applying Bayesian learning to large datasets. Examples include sampling such as \cite{ahn:distributedMCMC, bardenet:MCMC}, distributional approximations including stochastic variational inference (SVI) \cite{hoffman:svi} and assumed density filtering (ADF) \cite{miguel:pbp}, and approaches that mix distributional and sampling approximations \cite{gelman:dep,xu:sms}. 
%
One family of approximation method has garnered less attention in this regard: Expectation Propagation (EP) \cite{minka:ep, opper:ec}. EP constructs a posterior approximation by iterating simple local computations that refine factors which approximate the posterior-contribution from each datapoint. At first sight, it therefore appears well suited to large-data problems: the locality of computation make the algorithm simple to parallelise and distribute, and good practical performance on a range of small-data applications suggest that it will be accurate \cite{kuss:gpep,barthelme:ep_likelihood,cunningham:gaussianEP}. 
%
\hl{However the elegance of local computation has been bought at the price of prohibitive memory overhead that grows with the number of datapoints $N$, since each local approximating factor typically has the same complexity as the global approximation. The same pathology exists for the broader class of power EP (PEP) algorithms } \cite{minka:powerep} \hl{that includes variational message passing} \cite{winn:vmp}. \hl{In contrast, variational inference (VI) methods} \cite{jordan:variational,beal:variational} \hl{utilise global approximations that are refined directly, which prevents memory overheads from scaling with $N$. }

%On the other hand, there is a critical computational bottleneck in the standard EP approximation: each local approximating factor typically has the same complexity as the global approximation. This means that the EP algorithm has a memory overhead that grows with the number of data-points $N$, which makes it hard to scale rich models with many parameters to large-data settings (consider approximating the posterior distribution of a topic model with millions of parameters on a corpus containing billions of documents). The same pathology exists for the broader class of power EP (PEP) algorithms \cite{minka:powerep} that includes variational message passing \cite{winn:vmp}. The elegance of local computation has been bought at the price of having myriad local approximating factors. In contrast, variational inference (VI) methods \cite{jordan:variational,beal:variational} utilise global approximations that are refined directly (rather than through local components) which prevents memory overheads from scaling with $N$. 

Is there ever a case for preferring EP (or PEP) to VI methods for large-data?  We believe that there certainly is. First, EP can provide significantly more accurate approximations. It is well known that variational free-energy approaches are biased and often severely so \cite{turner+sahani:2011a} and for particular models the variational free-energy objective is pathologically ill-suited \cite{cunningham:gaussianEP,turner+sahani:2011c}. Second, the fact that EP is truly local (to factors in the posterior distribution and not just likelihoods) means that it affords different opportunities for tractable algorithm design, as the updates can be simpler to approximate.
%(the probabilistic back-propagation setting treated later in the paper is one such case where PEP is tractable, but variational free-energy methods require a second level of approximation\todo[fancyline]{double check with Miguel that this is correct}). 

As EP appears to be the method of choice for some applications, researchers have attempted to push it to scale. One approach is to swallow the large computational burden and simply use large data-structures to store the approximating factors (e.g.~TrueSkill \cite{herbrich:trueskill}). This approach can only be pushed so far. A second approach is to use \hl{ADF as a simple variant}, %of EP called assumed density filtering (ADF) 
which only requires a global approximation to be stored \cite{maybeck:adf}. ADF, however, provides poorly calibrated uncertainty estimates \cite{minka:ep} which was one of the main motivating reasons for developing EP in the first place. 
A third idea, complementary to the one described here, is to use approximating factors that have simpler structure (e.g.~low rank, \cite{qi+minka:sparseGP}). This reduces memory consumption (e.g.~for Gaussian factors from $\mathcal{O}(ND^2)$ to $\mathcal{O}(ND)$), but does not stop the scaling with $N$. Another idea uses EP to carve up the dataset \cite{gelman:dep,xu:sms} using approximating factors for collections of data-points. This results in coarse-grained, rather than local, updates and other methods must be used to compute them. (Indeed, the spirit of \cite{gelman:dep,xu:sms} is to extend sampling methods to large-datasets, not EP itself.) 

\hl{Can we have the best of both worlds? That is, accurate global approximations that are derived from truly local computation. To address this question we develop an algorithm based upon the standard EP and ADF algorithms that maintains a global approximation which is updated in a local way.}
%The question addressed in this paper is: can we have the best of both worlds? That is, accurate global approximations that are derived from truly local computation. For this purpose we develop an algorithm based upon the standard EP and ADF algorithms that maintains a global approximation which is updated in a local way. 
%
We call this class of algorithms Stochastic Expectation Propagation (SEP) since it updates the global approximation with (damped) stochastic estimates on data sub-samples in an analogous way to SVI. Indeed, the generalisation of the algorithm to the PEP setting directly relates to SVI. 
%
\hl{Importantly, SEP reduces the memory footprint by a factor of $N$ when compared to EP. We further extend the method to control the granularity of the approximation, and to treat models with latent variables without compromising on accuracy or unnecessary memory demands. Finally, we demonstrate the scalability and accuracy of the method on a number of real world and synthetic datasets.}

%Tests on synthetic data indicate that SEP performs almost as well as full EP, but reduces the memory footprint by a factor of $N$, and that it has well calibrated uncertainty estimates, unlike ADF. We show how to extend the method to treat models with latent variables without compromising on accuracy or memory demands. Finally, we demonstrate the scalability and accuracy of the method on a number of real world and synthetic datasets.


