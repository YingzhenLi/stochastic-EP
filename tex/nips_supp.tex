\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm, subfigure}
\usepackage{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\newtheorem{theorem}{Theorem}

\mathchardef\mhyphen="2D

\title{Stochastic Expectation Propagation: Supplementary Material}


\author{
Yingzhen Li \\
Department of Engineering\\
University of Cambridge\\
Cambridge, CB2 1PZ, UK \\
\texttt{yl494@cam.ac.uk} \\
\And
Richard E.~Turner \\
Affiliation \\
Address \\
\texttt{ret26@cam.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\appendix

\section{Further Theoretical Results}
We described the extensions of stochastic expectation propagation (SEP) in the main text, and we provide more details in this section.
\subsection{Power EP}
The analysis of EP and variational inference (VI) relationships asks for an introduction of power EP (PEP). As a preparation let us consider the alpha-divergence first introduced in \cite{amari:ig1985}
\begin{equation}
D_{\alpha}(p(\bm{\theta}) || q(\bm{\theta})) = \frac{4}{1 - \alpha^2} 
		\left( 1 - \int_{\bm{\theta}} p(\bm{\theta})^{ \frac{1+\alpha}{2}} q(\bm{\theta})^{ \frac{1- \alpha}{2}} d\bm{\theta} \right).
\end{equation} 
The two cases of KL-divergence also belongs to the family of alpha-divergence by definition:
\begin{align}
D_{1}(p(\bm{\theta})||q(\bm{\theta})) &\buildrel\triangle\over = \lim_{\alpha \rightarrow 1} D_{\alpha}(p(\bm{\theta})||q(\bm{\theta}))  = KL(p(\bm{\theta}) || q(\bm{\theta})), \\
D_{ \mhyphen 1}(p(\bm{\theta})||q(\bm{\theta})) &\buildrel\triangle\over = \lim_{\alpha \rightarrow \mhyphen 1} D_{\alpha}(p(\bm{\theta}) || q(\bm{\theta}))  = KL(q(\bm{\theta}) || p(\bm{\theta})).
\end{align}
\cite{minka:powerep} introduced alpha-EP as a generalisation of EP to alpha-divergences, which changes the moment matching step to alpha-projection $\alpha \mhyphen \mathtt{proj}[\cdot]$ \cite{amari:alpha_proj} that returns the minimiser of the alpha divergence $D_{\alpha}(\tilde{p}_n(\bm{\theta}) || q(\bm{\theta}))$ wrt.~$q(\bm{\theta})$ in $\mathcal{Q}$. Examples include the previous defined moment projection $\mathtt{proj}[\cdot]$ which takes $\alpha = 1$, and information projection which chooses $\alpha = -1$. 

Now we introduce the PEP algorithm with power $1 / \beta$ as a practical alternative of alpha-EP with $\alpha = -1 + 2 / \beta$ \footnote{Alpha-projection, except $\alpha = \pm 1$, is difficult to solve directly in general.}. Precisely PEP uses moment projection $\mathtt{proj}[q(\bm{\theta}) (p(\bm{x}_n|\bm{\theta}) / f_n(\theta))^{1/\beta}]$ and the stationary point of this iterative procedure is also the fixed points of the corresponded alpha-divergence. 
%
We summarise PEP, using the form of alpha-EP, in Algorithm \ref{alg:pep}. We further extend the alpha-projection idea to specify stochastic PEP in Algorithm \ref{alg:spep}. Likewise SEP/AEP corresponds to stochastic/averaged PEP with $\alpha = 1$, and the ``variational'' SEP/AEP chooses $\alpha = -1$. The latter case is often derived using variational techniques, and we further link the ``variational'' SEP/AEP with variational inference in the next section.

\begin{figure}[!t]
% UGLY USE OF \vspace & \hspace follows
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{algorithm}[H] 
\caption{PEP} \small
\label{alg:pep} 
\begin{algorithmic}[1] 
	\STATE choose a factor $f_n$ to refine:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) \propto q(\bm{\theta}) / f_n(\bm{\theta})$ 
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{\alpha \mhyphen proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-n}(\bm{\theta}) f_n(\bm{\theta})$\\\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
\quad
%
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{algorithm}[H]
\caption{Stochastic PEP} \small
\label{alg:spep} 
\begin{algorithmic}[1] 
%\STATE initialize $\{\tilde{f}_a\}$
	\STATE choose a datapoint $\bm{x}_n\sim \mathcal{D}$:
	\STATE compute cavity distribution \\ $q_{-1}(\bm{\theta}) \propto q(\bm{\theta}) / f(\bm{\theta})$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-1}(\bm{\theta})$
	\STATE moment matching: \\\hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{\alpha \mhyphen proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-1}(\bm{\theta}) f_n(\bm{\theta})$
	\STATE \textit{implicit update}:\\ $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \frac{1}{N}} f_n(\bm{\theta})^{\frac{1}{N}}$
\end{algorithmic}
\end{algorithm}
\end{minipage} 
%
\end{figure}


\subsection{Equivalence between the ``Variational'' AEP and Variational Inference}
We briefly discuss the equivalence between ``variational'' AEP and variational inference.  
%
Without loss of generality we provide a proof on exponential families. 

We first define the exponential families for prior, likelihood terms and the approximate posterior,
\begin{align}
\mathcal{P}_0 &= \{p_0(\bm{\theta}) = \exp \left( \langle \bm{\epsilon}_0, \gamma(\bm{\theta}) \rangle - A(\bm{\epsilon}_0) \right) |\ \bm{\epsilon}_0 \in \bm{E} \ s.t.~ A(\bm{\epsilon}_0) < \infty \}, \\
\mathcal{P} &= \{p(\bm{x} | \bm{\theta}) = \exp \left( \langle \beta(\bm{\theta}), \psi(\bm{x}) \rangle - A(\bm{\theta}) \right) |\ \bm{\theta} \in \bm{\Theta} \ s.t.~ A(\bm{\theta}) < \infty \}, \\
\mathcal{Q} &= \{q(\bm{\theta}) = \exp \left( \langle \bm{\lambda}_q, \phi(\bm{\theta}) \rangle - A(\bm{\lambda}_q) \right) |\ \bm{\lambda}_q \in \bm{\Lambda} \ s.t.~ A(\bm{\lambda}_q) < \infty \},
\end{align}
where $A(\cdot)$ represents the log partition function to guarantee a valid distribution. We denote the natural parameters of the approximating factor $f(\bm{\theta})$, and the intermediate answers $f_n(\bm{\theta})$ as $\bm{\lambda}$ and $\bm{\lambda}_n$, respectively. In EP we often use $\mathcal{P}_0 \subset \mathcal{Q}$ and in that case we also write $\bm{\lambda}_0$ as the natural parameter of $p_0$ when considering it in family $\mathcal{Q}$.

We next write the exponential family containing the true posterior and the tilted distribution:
\begin{equation}
\mathcal{\tilde{P}} = \{p(\bm{\theta}) = \exp \left( \langle \bm{\eta}, \Phi(\bm{\theta}) \rangle - A(\bm{\eta}) \right) |\ \bm{\eta} \in \bm{H} \ s.t.~ A(\bm{\eta}) < \infty \}, \\
\end{equation}
with sufficient statistics $\Phi(\bm{\theta}) = [\gamma(\bm{\theta}), \phi(\bm{\theta}), \beta(\bm{\theta}), A(\bm{\theta})]$ \footnote{We assume $\phi$, $\beta$ and $A$ are linearly independent wrt.~operation $E_{\tilde{p}}[\cdot], \forall \tilde{p} \in \tilde{\mathcal{P}}$ w.l.o.g.}. It is straight-forward that the true posterior has natural parameter $\bm{\eta}_p = [\bm{\epsilon}_0, \bm{0},\sum_{n} \psi(\bm{x}_n), -N]$, and the tilted distributions $\tilde{p}_n(\bm{\theta})$ has $\bm{\eta}_n = [\bm{0}, \bm{\lambda}_{-1}, \psi(\bm{x}_n), -1]$. We also write $\bm{\lambda}_q \leftarrow \mathtt{proj}[\bm{\eta}]$ to denote the moment projection of distributions in $\mathcal{\tilde{P}}$ to $\mathcal{Q}$.
%
We recall the AEP algorithm with current notations, with no-damped updates,
%
\begin{align}
\bm{\lambda}_{-1} & = \bm{\lambda}_0 + \frac{N-1}{N} \sum_{n=1}^N \bm{\lambda}_n, \\
\bm{\lambda}_n &= \mathtt{\alpha \mhyphen proj}[\bm{\eta}_n] - \bm{\lambda}_{-1}, \\
\bm{\lambda}_q &= \bm{\lambda}_0 + \sum_{n=1}^N \bm{\lambda}_n.
\end{align}
%
%
AEP with variational KL picks $\alpha = -1$. Now we state the following theorem. 
%
\begin{theorem}
The ``variational'' averaged expectation propagation algorithm, when converged to a fixed point, returns a local optimum of variational inference.
\end{theorem}
\begin{proof}
We first consider the case that $\mathcal{P}_0 \subset \mathcal{Q}$, which means $\bm{\lambda}_0$ is a fixed vector corresponding to the prior term. VI directly optimises the global KL divergence wrt.~$q(\bm{\theta})$:
\begin{equation}
KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \langle \bm{\lambda}_q, E_q \phi(\bm{\theta}) \rangle - A(\bm{\lambda}_q) - \langle \bm{\eta}_p, E_q \Phi(\bm{\theta}) \rangle + A(\bm{\eta}_p).
\end{equation}
Using $\bm{\eta}_p = [\bm{\lambda}_0, \sum_{n} \psi(\bm{x}_n), -N]$,
\begin{equation}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \langle \bm{\lambda}_q - \bm{\lambda}_0, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \sum_{n} \psi(\bm{x}_n), \nabla_{\bm{\lambda}_q} E_q \beta(\bm{\theta}) \rangle + N \nabla_{\bm{\lambda}_q} E_q A(\bm{\theta}).
\end{equation}
Where $\bm{\lambda}_q - \bm{\lambda}_0 = \sum_n \bm{\lambda}_n$ by definition. On the other hand, the inner loop of the ``variational'' AEP finds the optimiser of the following divergence
\begin{equation}
KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})) = \langle \bm{\lambda}_q, E_q \phi(\bm{\theta}) \rangle - A(\bm{\lambda}_q) - \langle \bm{\eta}_n, E_q \Phi(\bm{\theta}) \rangle + A(\bm{\eta}_n).
\end{equation}
We treat the cavity parameter $\bm{\lambda}_{-1}$ as a constant in this optimisation. Similarly we obtain the graident wrt.~$\bm{\lambda}_q$
\begin{equation}
\nabla_{\bm{\lambda}_q}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})) = \langle \frac{1}{N} \sum_n \bm{\lambda}_n, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \psi(\bm{x}_n), \nabla_{\bm{\lambda}_q} E_q \beta(\bm{\theta}) \rangle +  \nabla_{\bm{\lambda}_q} E_q A(\bm{\theta}).
\end{equation}
Now since the inner-loop for datapoint $\bm{x}_n$ only contributes to the intermediate answer $\bm{\lambda}_n$ by definition, if we view the ``variational'' AEP as a gradient descent method, we have the final gradient of the global parameter constructed by summing the gradients on the local KL divergences. Readers can verify that the gradient of the global parameter is equivalent to the gradient of VI objective, i.e.
\begin{equation}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \sum_{n=1}^N \nabla_{\bm{\lambda}_q}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})).
\end{equation}
This implies that the fixed points of ``variational'' AEP are also stationary points of VI.

We next consider the general case where $\mathcal{P}_0 \neq \mathcal{Q}$, which means we need to update $\bm{\lambda}_0$ in parallel as well. Now the gradient in VI case changes to
\begin{equation}
\begin{aligned}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) &= \langle \bm{\lambda}_q, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \bm{\epsilon}_0, \nabla_{\bm{\lambda}_q} E_q \gamma(\bm{\theta}) \rangle \\
	&- \langle \sum_{n} \psi(\bm{x}_n), \nabla_{\bm{\lambda}_q} E_q \beta(\bm{\theta}) \rangle + N \nabla_{\bm{\lambda}_q} E_q A(\bm{\theta}).
\end{aligned}
\end{equation}
On the other hand, the ``variational'' AEP also approximate the prior $p_0(\bm{\theta})$ by updating $\bm{\lambda}_0$ with normal EP. The cavity distribution is computed by $\bm{\lambda}_{-0} = \bm{\lambda}_q - \bm{\lambda}_0$. We insert the prior to form the tilted distribution $\tilde{p}_0(\bm{\theta}) \propto q_{-0}(\bm{\theta}) p_0(\bm{\theta})$, and minimise the moment matching KL by noticing $\bm{\eta}_0 = [\bm{\epsilon}_0, \bm{\lambda}_{-0}, 0, 0]$ and new update:
\begin{equation}
\nabla_{\bm{\lambda}_q}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_0(\bm{\theta})) = \langle \bm{\lambda}_0, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \bm{\epsilon}_0, \nabla_{\bm{\lambda}_q} E_q \gamma(\bm{\theta}) \rangle.
\end{equation}
Finally readers can verify the following equivalence as well
\begin{equation}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \sum_{n=0}^N \nabla_{\bm{\lambda}_q}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})),
\end{equation}
which finishes the proof.
\end{proof}

%
\subsection{SEP from a Global Approximation Perspective}
We relate SEP to a global approximation algorithm though it is computed in a truely local way. This framework utilises alpha divergence but on the global posterior, and we motivate it by describing VI and SVI as divergence minimisation.
%
VI performs global optimisation on $KL(q(\bm{\theta})||p(\bm{\theta}|\mathcal{D}))$, and its stochastic version, SVI, is driven by the gradients of $KL(q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_n\}^N))$ on the $N$ replicates $\{\bm{x}_n\}^N$. Similarly, we state SEP as a stochastic global optimisation procedure, which minimises the $\alpha$-divergence $D_{\alpha}(p(\bm{\theta} | \{\bm{x}_n\}^N) || q_n(\bm{\theta}))$ with $\alpha = \mhyphen 1 + 2/N$. Indeed we can understand the inner-loop of AEP as PEP with power $\beta = 1/N$ if considering $f(\bm{\theta})^N$ as the ``big factor'' to approximate the likelihood term of $\bm{x}_n$ raised to power $N$.

However minimising the alpha-divergence between the true posterior $p(\bm{\theta}|\mathcal{D})$ and the global approximation $q(\bm{\theta})$ recovers PEP on the whole dataset instead, and the factor to include in the tilted distribution changes to the intractable $\mathtt{avg}[\{p(\bm{x}_n | \bm{\theta}) \}]$. Readers might have noticed that the update of PEP on full dataset is given by $q(\bm{\theta}) \leftarrow \mathtt{proj}[\mathtt{avg}[\{ \tilde{p}_n(\bm{\theta}) \}]]$. In other words, we can interpret AEP as an approximation to the impractical batch PEP by interchanging computations, and we illustrate a geometric view for this in Figure \ref{fig:aep_vs_pep}.

It is important to note that SEP/AEP in convergence does not minimise the alpha divergence globally. Like PEP, the inner-loop compute $\mathtt{proj}[\tilde{p}_n(\bm{\theta})]$, where one can show that it moves towards minimising $D_{\alpha}(p(\bm{\theta} | \{\bm{x}_n\}^N) || q_n(\bm{\theta}))$ using the same techniques as before. However the outer-loop averages the natural parameters of the intermediate answers, which breaks the optimisation in the inner-loop. Further local/global optimisation of alpha divergence are inconsistent in terms of fixed point except $\alpha = -1$, the divergence utilised in VI and VMP. Indeed we provide the fixed points conditions of AEP which shows its local approximation nature.
%
\begin{theorem}
The fixed points of averaged EP, if exist, can be written as $q(\bm{\theta}) = \mathtt{avg}[\{q_n(\bm{\theta})\}]$, where
\begin{equation}
q_n(\bm{\theta}) = \mathtt{proj}[\tilde{p}_n(\bm{\theta})].
\label{eq:mm}
\end{equation}
These fixed points are also the fixed points of stochastic EP in expectation. 
\end{theorem}
\begin{proof}
In each update SEP gives the same answers as AEP in expectation if initialised at the same starting point. Also as an analogy to normal EP, the stationary points of AEP ask for moment matching between tilted and intermediate distributions. 
\end{proof}
%

\begin{figure}
\centering
\def\svgwidth{0.35\linewidth}
\subfigure[\label{fig:aep_vs_pep}]{
\input{fig/aep_vs_pep.pdf_tex}}
%
\caption{(a) A geometric view of AEP/PEP comparison}
\end{figure}

\section{Further Experimental Results}
We further test the performance of SEP with sampling methods to compute moments \footnote{code adjusted from \texttt{ep-stan}: \url{https://github.com/gelman/ep-stan}}. We re-use the settings of probit regression but change the probit unit to sigmoid function, making the M-projection analytically intractable. We partition the dataset into $K = 20$ subsets $\{\mathcal{D}_k\}$ and consider the true posterior as
\begin{equation}
p(\bm{\theta}|\mathcal{D}) \propto p_0(\bm{\theta}) \prod_{k=1}^K p(\mathcal{D}_k|\bm{\theta}).
\end{equation}
We construct the approximate posterior with local factors over the subsets and tied them in SEP/AEP as before. Again as presented in Figure \ref{fig:sep_logit}, SEP performs almost as well as EP, which further justifies SEP even with sampling methods. Also AEP is indistinguishable from DEP, but it reduces memory by a factor of $K$.

\begin{figure}
\centering
%
\def\svgwidth{0.3\linewidth}
\subfigure[\label{fig:sep_logit}]{
\input{fig/sep_logit.pdf_tex}}
%
\caption{Performance of EP methods on Bayesian logistic regression, where we use probit function for (a)(b) and sigmoid function for (c).}
\end{figure}

\subsubsection*{References}
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{nips_sep}


\end{document}
