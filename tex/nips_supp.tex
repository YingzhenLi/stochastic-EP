\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm, subfigure}
\usepackage{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage{color,soul}

\newcommand{\ica}{\hspace{0.25cm}}
\renewcommand{\arraystretch}{0.94}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}

\mathchardef\mhyphen="2D

\title{Stochastic Expectation Propagation: Supplementary Material}


\author{
Yingzhen Li \\
University of Cambridge\\
Cambridge, CB2 1PZ, UK \\
\texttt{yl494@cam.ac.uk} \\
\And
Jos\'e Miguel Hern\'andez-Lobato\\
Harvard University \\
Cambridge, MA 02138 USA \\
\texttt{jmh@seas.harvard.edu}
\And
Richard E.~Turner \\
University of Cambridge\\
Cambridge, CB2 1PZ, UK \\
\texttt{ret26@cam.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\appendix

\section{Further theoretical results}
We described the extensions of stochastic expectation propagation (SEP) in the main text, and we provide more details in this section.

\subsection{Power EP and alpha-EP}
The analysis of EP and variational inference (VI) relationships asks for an introduction of power EP (PEP). As a preparation let us consider the alpha-divergence\footnote{\hl{A little math can show the updates of alpha-EP using different existing alpha-divergence definitions are equivalent, although the corresponding alpha may be different.}} introduced in \cite{amari:ig1985}
\begin{equation}
D_{\alpha}[p(\bm{\theta}) || q(\bm{\theta})] = \frac{4}{1 - \alpha^2} 
		\left( 1 - \int_{\bm{\theta}} p(\bm{\theta})^{ \frac{1+\alpha}{2}} q(\bm{\theta})^{ \frac{1- \alpha}{2}} d\bm{\theta} \right).
\end{equation} 
The two cases of KL-divergence also belongs to the family of alpha-divergence by definition:
\begin{align}
D_{1}[p(\bm{\theta})||q(\bm{\theta})] &\buildrel\triangle\over = \lim_{\alpha \rightarrow 1} D_{\alpha}[p(\bm{\theta})||q(\bm{\theta})]  = \mathrm{KL}[p(\bm{\theta}) || q(\bm{\theta})], \\
D_{ \mhyphen 1}[p(\bm{\theta})||q(\bm{\theta})] &\buildrel\triangle\over = \lim_{\alpha \rightarrow \mhyphen 1} D_{\alpha}[p(\bm{\theta}) || q(\bm{\theta})]  = \mathrm{KL}[q(\bm{\theta}) || p(\bm{\theta})].
\end{align}
\cite{minka:powerep} introduced alpha-EP as a generalisation of EP to alpha-divergences, which changes the moment matching step to alpha-projection \cite{amari:alpha_proj} that returns the minimiser of the alpha divergence $D_{\alpha}[\tilde{p}_n(\bm{\theta}) || q(\bm{\theta})]$ wrt.~$q(\bm{\theta})$ in $\mathcal{Q}$. Examples include moment projection $\mathtt{proj}[\cdot]$ which takes $\alpha = 1$, and information projection which chooses $\alpha = -1$. However alpha-projections are difficult to compute in general, motivating power EP (Algorithm \ref{alg:pep}) as a practical alternative. \cite{minka:powerep} showed that power EP with power $1 / \beta, \beta < \infty$ returns a local optimum of the alpha divergence with $\alpha = -1 + 2 / \beta$ when converged. 
%
\hl{However this still leaves the pathological case $\alpha = -1$ or $\beta = \infty$ since the above equivalence does not apply. Thus variatioinal message passing (VMP), which takes $\alpha \rightarrow -1$, cannot be interpreted as a special case of power EP.} This observation extends to stochastic PEP as well (Algorithm \ref{alg:spep}). Instead we derive stochastic VMP in the spirit that SEP extends EP, which keeps the computational steps using current global estimate but tie all the local factors. We discuss this extension in detail in the next section and provide its connection to stochastic variational inference.

\begin{figure}[!t]
% UGLY USE OF \vspace & \hspace follows
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{algorithm}[H] 
\caption{PEP} \small
\label{alg:pep} 
\begin{algorithmic}[1] 
	\STATE choose a factor $f_n$ to refine:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) \propto q(\bm{\theta}) / f_n(\bm{\theta})^{1/\beta}$ 
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta})^{1 / \beta} q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow [\mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta})]^{\beta} $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q(\bm{\theta}) f_n(\bm{\theta}) / f_n^{old}(\bm{\theta})$\\
	\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
\quad
%
\begin{minipage}[t]{0.45\linewidth}
\centering
\begin{algorithm}[H]
\caption{Stochastic PEP} \small
\label{alg:spep} 
\begin{algorithmic}[1] 
%\STATE initialize $\{\tilde{f}_a\}$
	\STATE choose a datapoint $\bm{x}_n\sim \mathcal{D}$:
	\STATE compute cavity distribution \\ $q_{-1}(\bm{\theta}) \propto q(\bm{\theta}) / f(\bm{\theta})^{1/\beta}$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta})^{1/\beta} q_{-1}(\bm{\theta})$
	\STATE moment matching: \\\hspace{-1mm}$f_n(\bm{\theta}) \leftarrow [\mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta})]^{\beta} $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q(\bm{\theta}) f_n(\bm{\theta}) / f(\bm{\theta})$
	\STATE \textit{implicit update}:\\ $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \frac{1}{N}} f_n(\bm{\theta})^{\frac{1}{N}}$
\end{algorithmic}
\end{algorithm}
\end{minipage} 
%
\end{figure}

\subsection{Connecting SVMP to SVI}
We first briefly sketch the VMP algorithm using EP framework but replacing the moment matching step with natural parameter matching. We assume the approximate posterior $q(\bm{\theta})$ is in some exponential family: 
\begin{equation}
q(\bm{\theta}) \propto \exp \left[ \langle \bm{\lambda}_q, \bm{\phi}(\bm{\theta}) \rangle \right].
\end{equation}
At time $t$ we have the current estimate of the natural parameter $\bm{\lambda}_q^t$, which is defined as the sum of local variational parameters\footnote{This notation implicitly assumes that the prior also belongs to the approximate distribution family $\mathcal{Q}$. In general we can propose another factor to approximate $p_0(\bm{\theta})$, and our result still applies.}:
%
$\bm{\lambda}_q^t \buildrel\triangle\over = \bm{\lambda}_0 + \sum_{n=1}^N \bm{\lambda}_n^t$.
%
Here $\bm{\lambda}_0$ represents the natural parameter of the prior distribution $p_0(\bm{\theta})$. VMP iteratively computes the update of each local estimate $\bm{\lambda}_n^{t+1}$ in the following procedure. First VMP computes the expected sufficient statistics $\hat{\bm{s}}_n$ about datapoint $\bm{x}_n$ using $\bm{\lambda}_q^t$, \hl{e.g.~$\hat{\bm{s}}_n = E_{q}[t(z_n, x_n)]$ in the original SVI paper} \cite{hoffman:svi}. Then VMP forms the gradient as though optimising the maximised evidence lower bound (ELBO) but with $q_{-1}(\bm{\theta})$ as the prior:
\begin{align}
\nabla_{\bm{\lambda}_q^t} \mathcal{L} &= \bm{\lambda}_{-1}^t + \hat{\bm{s}}_n - \bm{\lambda}_q^t, \\
\bm{\lambda}_{-1}^t &= \bm{\lambda}_q^t - \bm{\lambda}_{n}^t.
\end{align}
Next VMP zeros the gradient and recovers the current update $\bm{\lambda}_n^{t+1} = \hat{\bm{s}}_n$. The stochastic version of VMP, if extended in a way as SEP developed from EP, defines the global variational parameters as $\bm{\lambda}_q^t \buildrel\triangle\over = \bm{\lambda}_0 + N \bm{\lambda}^t$. It computes the expected sufficient statistics $\hat{\bm{s}}_n$ in the same way as VMP but changes the cavity to $\bm{\lambda}_{-1}^t = \bm{\lambda}_q^t - \bm{\lambda}^t$ in the ELBO maximisation steps. Readers can verify that this returns the current update $\bm{\lambda}^{t+1} = \hat{\bm{s}}_n$ \hl{using the important fact that the local update ONLY depends on the global parameter $\bm{\lambda}_q^t$.} Now since we tie all the local updates, the global parameter update $\bm{\lambda}_q^{t+1} = \bm{\lambda}_0 + N \bm{\lambda}^{t+1} = \bm{\lambda}_0 + N \hat{\bm{s}}_n$. In practice we perform a damped update, where a typical choice of step size is $\epsilon = 1/N$ like in SEP:
\begin{equation}
\bm{\lambda}_q^{t+1} \leftarrow (1 - \frac{1}{N}) \bm{\lambda}_q^t + \frac{1}{N}(\bm{\lambda}_0 + N \bm{\lambda}^{t+1}) = \bm{\lambda}_0 + (N-1) \bm{\lambda}^t + \hat{\bm{s}}_n.
\end{equation} 

On the other hand, \cite{mandt:smoothedSVI} summarises stochastic variational inference (SVI) as to compute the current update by zeroing the gradient
\begin{equation}
\nabla_{\bm{\lambda}_q} \mathcal{L} = \bm{\lambda}_0 + N \hat{\bm{s}}_n - \bm{\lambda}_q,
\end{equation}
which returns $\bm{\lambda}_q^{t+1} = \bm{\lambda}_0 + N \hat{\bm{s}}_n$ as well. This implies that SVI, when using learning rate $\epsilon = 1/N$, is equivalent to SVMP. 


%
\subsection{SEP from a global approximation perspective}
We relate SEP to a global approximation algorithm though it is computed in a truely local way. This framework utilises alpha divergence but on the global posterior, and we motivate it by describing VI and SVI as divergence minimisation.
%
VI performs global optimisation on $\mathrm{KL}[q(\bm{\theta})||p(\bm{\theta}|\mathcal{D})]$, and its stochastic version, SVI, can be interpreted as \hl{in each step minimising} $\mathrm{KL}[q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_n\}^N)]$ with the $N$ replicates $\{\bm{x}_n\}^N$. Similarly, we state SEP as a stochastic global optimisation procedure, which computes an iterative procedure to minimise alpha-divergence $D_{\alpha}[p(\bm{\theta} | \{\bm{x}_n\}^N) || q(\bm{\theta})]$ with $\alpha = \mhyphen 1 + 2/N$. Indeed we can understand the inner-loop of AEP as PEP with power $1/N$ if considering $f(\bm{\theta})^N$ as the ``big factor'' to approximate the likelihood term of $\bm{x}_n$ raised to power $N$.

However minimising the alpha-divergence between the true posterior $p(\bm{\theta}|\mathcal{D})$ and the global approximation $q(\bm{\theta})$ recovers PEP on the whole dataset instead, and the factor to include in the tilted distribution changes to the intractable geometric average $\mathtt{avg}[\{p(\bm{x}_n | \bm{\theta}) \}] \buildrel\triangle\over = [\prod_n p(\bm{x}_n | \bm{\theta}) ]^{1/N} $. Readers might have noticed that the update of PEP on full dataset is given by $q(\bm{\theta}) \leftarrow \mathtt{proj}[\mathtt{avg}[\{ \tilde{p}_n(\bm{\theta}) \}]]$. In other words, we can interpret AEP as an approximation to the impractical batch PEP by interchanging computations, and we illustrate a geometric view for this in Fig. \ref{fig:aep_vs_pep}.

It is important to note that SEP/AEP in convergence does not minimise the alpha divergence globally. Like PEP, the inner-loop compute $\mathtt{proj}[\tilde{p}_n(\bm{\theta})]$, where one can show that it moves towards minimising $D_{\alpha}(p(\bm{\theta} | \{\bm{x}_n\}^N) || q_n(\bm{\theta}))$ using the same techniques as before. However the outer-loop averages the natural parameters of the intermediate answers, which \hl{does not follow the gradient direction of alpha-divergence minimisation}. Furthermore, local/global optimisation of alpha divergence are inconsistent in terms of fixed point except $\alpha = -1$, the divergence utilised in VI and VMP. Indeed we provide the fixed points conditions of AEP which shows its local approximation nature.
%
\begin{prop}
The fixed points of averaged EP, if exist, can be written as $q(\bm{\theta}) = \mathtt{avg}[\{q_n(\bm{\theta})\}]$, where
\begin{align}
q_n(\bm{\theta}) &= \mathtt{proj}[\tilde{p}_n(\bm{\theta})], \\
\tilde{p}_n(\bm{\theta}) & \propto q(\bm{\theta}) \frac{p(\bm{x}_n|\bm{\theta})}{f(\bm{\theta})}.
\end{align}
These fixed points are also the fixed points of stochastic EP \hl{in expectation}. 
\end{prop}
%
This fixed point condition applies to stochastic PEP as well when $\alpha \neq -1$, and importantly it also implies the pathology of constructing SVMP by \hl{using SPEP} and limiting $\alpha$ to $-1$. 

%
\begin{figure}
\centering
\def\svgwidth{0.35\linewidth}
\subfigure[\label{fig:aep_vs_pep}]{
\input{fig/aep_vs_pep.pdf_tex}}
%
\hspace{0.5in}
%
\def\svgwidth{0.4\linewidth}
\subfigure[\label{fig:dep_sep_dsep}]{
\input{fig/dep_sep_dsep.pdf_tex}}

\caption{(a) A geometric view of AEP/PEP comparison. (b) A cartoon illustration of DEP, SEP and DSEP. For each algorithm we show the approximate posterior on the top and the tilted distribution at the bottom.}

\end{figure}

\section{Algorithmic design details}
%
\subsection{Distributed computing methods}
We have shown in the main paper that a proper design of data partitioning improves SEP's approximation accuracy. This distributed algorithm is inspired by the Distributed EP (DEP) algorithm \cite{gelman:dep, xu:sms} presented in Algorithm \ref{alg:dep}. DEP first partitions the dataset into $K$ disjoint pieces $\{ D_k = \{\bm{x}_i\}_{i=1}^{N_k} \}$ with $N = \sum_{i=1}^K N_k$, which is well-justified since the true posterior can also be derived as
\begin{align}
p(\bm{\theta}|\mathcal{D}) &\propto p_0(\bm{\theta}) \prod_{k=1}^K p(\mathcal{D}_k|\bm{\theta}), \\
p(\mathcal{D}_k|\bm{\theta}) &= \prod_{\bm{x}_n \in \mathcal{D}_k} p(\bm{x}_n | \bm{\theta}).
\end{align}
%
Next DEP assigns factors to each sub-dataset likelihood, i.e.~$q(\bm{\theta}) \propto p_0(\bm{\theta}) \prod_k f_k(\bm{\theta})$ with each $f_k(\bm{\theta})$ approximating $p(\mathcal{D}_k|\bm{\theta})$. The projection step is no longer analytically tractable in general since the tilted distribution with multiple datapoints often lacks a simple form. Instead DEP handles moment matching with sampling, making it stochastic in the sense of moment approximation. 

%
To have a deterministic counterpart of DEP, we consider SEP/AEP inside each partition. We name this approach as Distributed SEP/AEP (DSEP/DAEP) and provide a comparison in Fig. \ref{fig:dep_sep_dsep} with DEP and SEP using sampling protocol. Different from DEP, the approximate posterior for DSEP is defined as $q(\bm{\theta}) \propto p_0(\bm{\theta}) \prod_k f_k(\bm{\theta})^{N_k}$, with $f_k(\bm{\theta})^{N_k}$ approximating $p(\mathcal{D}_k|\bm{\theta})$. The computations are almost the same as SEP/AEP except that the updates only modifies the copies of the corresponded subset.
%
These two algorithms are also detailed in Algorithm \ref{alg:dsep} and \ref{alg:daep}, respectively. \hl{In section C we provide an emprical study on comparing SEP, EP and DSEP approximations.}


\begin{figure}[!t]
% UGLY USE OF \vspace & \hspace follows
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{DEP} \small
\label{alg:dep} 
\begin{algorithmic}[1]
	\STATE compute cavity distribution \\$q_{-k}(\bm{\theta}) \propto q(\bm{\theta}) / f_k(\bm{\theta})$ 
	\STATE compute tilted distribution \\$\tilde{p}_k(\bm{\theta}) \propto p(\mathcal{D}_k|\bm{\theta}) q_{-k}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-3mm}$f_k(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_k(\bm{\theta})] / q_{-k}(\bm{\theta}) $ \\
\hspace{1mm}\\ \vspace{4.3mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{DSEP} \small
\label{alg:dsep} 
\begin{algorithmic}[1] 
	\STATE compute cavity distribution \\$q_{-1}(\bm{\theta}) = q(\bm{\theta}) / f_k(\bm{\theta})$
	\STATE choose a datapoint $\bm{x}_n \sim \mathcal{D}_k$
	\STATE compute tilted distribution \\$\tilde{p}_k^n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-1}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_k^n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_k^n(\bm{\theta})] / q_{-1}(\bm{\theta}) $
	\STATE inclusion:\\ \hspace{-5mm} $f_k(\bm{\theta}) \leftarrow f_k(\bm{\theta})^{1 - 1/N_k} f_k^n(\bm{\theta})^{1/N_k}$
\end{algorithmic}
\end{algorithm}
\end{minipage}
%\quad
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H]
\caption{DAEP} \small
\label{alg:daep} 
\begin{algorithmic}[1] 
%\STATE initialize $\{\tilde{f}_a\}$

	\STATE compute cavity distribution \\ $q_{-1}(\bm{\theta}) \propto q(\bm{\theta}) / f_k(\bm{\theta})$
	\STATE for each $\bm{x}_n \in \mathcal{D}_k$:
	\STATE \quad compute tilted distribution \\$\tilde{p}_k^n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-1}(\bm{\theta})$
	\STATE \quad moment matching: \\\hspace{-1mm}$f_k^n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_k^n(\bm{\theta})] / q_{-1}(\bm{\theta}) $
	\STATE inclusion:\\ $f_k(\bm{\theta})^{N_k} \leftarrow \prod_n f_k^n(\bm{\theta})$
\end{algorithmic}
\end{algorithm}
\end{minipage} 
%
\caption{Comparing the variants of distributed design for Expectation Propagation (EP) on the current data piece $\mathcal{D}_k$. One should notice that the definitions of $f_k(\bm{\theta})$ are different for DEP and DSEP/DAEP. Distributed EP (DEP) uses sampling methods to compute the projection step, while Distributed SEP/AEP (DSEP/DAEP) still keeps deterministic computations.}
\end{figure}

\subsection{SEP with latent variables}
In this section we show the applicability of SEP to latent variables without scaling the memory consumption with $N$.
%
We consider a model containing latent variables $\bm{h}_n$ associated with each observation $\bm{x}_n$, which are drawn i.i.d.~from a prior $p_0(\bm{h}_n)$. SEP proposes approximations to the true posterior over parameters and hidden variables 
\begin{equation}
p(\bm{\theta}, \{ \bm{h}_n\} | \mathcal{D}) \propto p_0(\bm{\theta}) \prod_n p_0(\bm{h}_n) p(\bm{x}_n | \bm{h}_n, \bm{\theta})
\end{equation}
by tying the factors for the global parameter $\bm{\theta}$ but retaining the local factors for the hidden variables:
%
\begin{equation}
q(\bm{\theta}, \{ \bm{h}_n\}) \buildrel\triangle\over \propto p_0(\bm{\theta}) f(\bm{\theta})^N \prod_{n=1}^N g_n(\bm{h}_n) .
\end{equation}
In other words, SEP uses $f(\bm{\theta}) g_n(\bm{h}_n)$ to approximate $p(\bm{x}_n | \bm{h}_n, \bm{\theta})p_0(\bm{h}_n)$.

Next we show a critical advantage of SEP on approximating latent variable posterior: the local factors $g_n(\bm{h}_n)$ do not need to be maintained in memory \hl{if it can be computed without special requirements for initialisations.} 
%
More formally, the cavity distribution is $q_{-n}(\bm{\theta}, \{ \bm{h}_n\}) \propto q(\bm{\theta}, \{ \bm{h}_n\})/(f(\bm{\theta}) g_n(\bm{h}_n)) $ and the tilted distribution is $\tilde{p}_n(\bm{\theta}, \{ \bm{h}_n\}) \propto q_{-n}(\bm{\theta}, \{ \bm{h}_n\}) p(\bm{x}_n | \bm{h}_n, \bm{\theta})p_0(\bm{h}_n)$. This leads to the a moment-update that minimises 
%
\begin{equation}
\mathrm{KL}[ p_0(\bm{\theta}) f(\bm{\theta})^{N-1} p(\bm{x}_n | \bm{h}_n, \bm{\theta})p_0(\bm{h}_n) \prod_{m\ne n} g_m(\bm{h}_m) || p_0(\bm{\theta}) f(\bm{\theta})^{N-1} f'(\bm{\theta}) g_n(\bm{h}_n) \prod_{m\ne n} g_m(\bm{h}_m)] .\nonumber
\end{equation}
%
with respect to $f'(\bm{\theta}) g_n(\bm{h}_n)$. Importantly, the terms involving $\prod_{m\ne n} g_m(\bm{h}_m)$ are cancelled, meaning that these factors \hl{do not contribute to the local approximation step. For simple models the moments of $\bm{h}_n$ can be computed analytically given $q_{-1}(\bm{\theta})$, thus $g_n(\bm{h}_n)$ is never stored in memory resulting in a reduced memory footprint by a factor of $N$ again. However in practice people may prefer maintaining the $g$ factors in memory, if the moment computation requires another optimisation inner-loop (which might be more expensive than the moment matching step itself). Examples include latent Dirichlet allocation} \cite{blei:lda} \hl{that has a hierachy of latent variables, where VI methods also store variational $q$ distributions for some of the hidden variables. One potential recipe in this scenario is to learn the moments/messages passed in each SEP step, where interested readers are directed to} \cite{heess:learning_messages, jitkrittum:kernel}.

It is also possible to have latent variables globally shared or shared in a data piece $\mathcal{D}_k$. But we can also extend SEP to these latent variables accordingly, which still provides computation gains in space complexity. In mathematical forms, assume $\bm{h}_k$ a latent variable shared in $\mathcal{D}_k$. Then we construct $q(\bm{h}_k) \propto p_0(\bm{h}_k) g_k(\bm{h}_k)^{N_k}$ to approximate its posterior. This procedure still reduces memory by a factor of $N/K$.

\section{Further experimental results}

\subsection{Details of Bayesian neural network tests}

We perform neural network regression experiments with publicly available data sets and neural networks with one hidden layer.  Table~\ref{tab:datasets_neural_networks} lists the analyzed data sets and shows summary statistics.  We use neural networks with 50 hidden units in all cases except in the two largest ones, i.e., \emph{Year Prediction
MSD} and \emph{Protein Structure}, where we use 100 hidden units. The different methods, SEP, EP and ADF were run by performing 40 passes over the available training data, updating the parameters of the posterior approximation after seeing each data point.  The data sets are split into random training and test sets with 90\% and 10\% of the data, respectively. This splitting process is repeated 20 times and the average test performance of each method is reported. In the two largest data sets, \emph{Year Prediction MSD} and \emph{Protein Structure}, we do the train-test splitting only one and five times respectively. The data sets are normalized so that the input features and the targets have zero mean and unit variance in the training set. The normalization on the targets is removed for prediction.

We also provide the memory consumption details for experiments using probabilistic back-propagation (PBP) in Table \ref{tab:datasets_neural_networks}. We observe substantial memory reductions by running SEP instead of EP, while still attaining similar accuracies. Especially for Year Prediction MSD dataset, which is a typical large-scale dataset both in the number of observations $N$ and the dimensionality $D$, SEP achieves tens of gigabytes savings. \hl{We performed the test for EP using a machine with more than 100GB RAM, while SEP only required 2.7GB memory, including the space of storing the dataset (1.9GB). These numbers reveal the incredibly huge memory requirement of full EP and further support SEP as a practical alternative in big data, big model settings.}


\begin{table} 
\caption{Datasets Used in the Experiments with Neural Networks. The memory numbers reported include dataset storage and temporal maintainance of computation graphs in Theano ($\sim100MB$).}
\label{tab:datasets_neural_networks} 
\centering 
\begin{tabular}{lrrrrr} 
\hline  
\textbf{Dataset} & $N$ & $D$ & MB (EP) & MB (SEP) & MB reduced \tabularnewline 
\hline 
Kin8nm & 8192 & 8 & 168.23 & 109.76 & 58.47 \tabularnewline 
Naval Propulsion & 11,934 & 16 & 261.75 & 113.92 & 147.83 \tabularnewline 
Combined Cycle Power Plant & 9568 & 4 & 148.70 & 110.99 & 37.71 \tabularnewline 
Protein Structure & 45,730 & 9 & 815.55 & 121.52 & 694.02 \tabularnewline 
Wine Quality Red & 1599 & 11 & 122.21 & 107.90 & 14.30 \tabularnewline 
\bf{Year Prediction MSD} & \bf{515,345} & \bf{90} & \bf{67837.90} & \bf{2730.55} & \bf{65107.34} \tabularnewline 
\hline 
\end{tabular}
\end{table}


\subsection{Stochastic EP with sampling protocal}

Although not a main purpuse, we further test the performance of SEP with sampling methods to compute moments \footnote{code adjusted from \texttt{ep-stan}: \url{https://github.com/gelman/ep-stan}}. We re-use the settings of probit regression but change the probit unit to sigmoid function, making the moment projection analytically intractable. We randomly partition the dataset into $K = 20$ subsets $\{\mathcal{D}_k\}$, construct the approximate posterior with local factors over the subsets, and tie them in SEP/AEP as before. \hl{Note that we perform sequential computations for DEP and AEP although they are ideally suited for paralel computing.} Again as presented in Figure \ref{fig:sep_logit}, SEP performs almost as well as EP, which further justifies SEP even with sampling methods. Also AEP is indistinguishable from DEP, but it reduces memory by a factor of $K$.

\begin{figure}
\centering
\def\svgwidth{0.3\linewidth}
\input{fig/sep_logit.pdf_tex}
\caption{Performance of EP methods on Bayesian logistic regression with sampling moment computations.}
\label{fig:sep_logit}
\end{figure}


\subsection{Further Comparisons for SEP, DSEP and full EP}
\hl{The assumption we made in the main text to achieve SEP $\approx$ full EP is that the contributions of each likelihood term to the posterior are very similar. We show further results here on the approximation produced by different EP methods when we believe there exists heterogeneity in data.}
%
\hl{We generated synthetic XOR classification data by sampling from 4 unit Gaussians with means $(3, 3)$, $(-3, -3)$, $(3, -3)$ and $(-3, 3)$, and labelling the clusters centered at the former two as negative examples (and positive for the others). The model $p(y_n|\bm{x}_n, \bm{\theta})$ is kernel probit regression using RBF kernel with width $l=1.0$, which is the same as the model presented in Section 5.1 in the main text except that the features are changed to kernel representations. This makes the feature vectors high dimensional, and the local nature of kernels also makes them very different if the datapoints belong to different clusters. We generated $50*4$ test data and $\{10*4, 20*4, 50*4\}$ training data and ran SEP/DSEP/full EP to approximate the posterior distribution of $\bm{\theta}$. For DSEP we partitioned the dataset into 4 subsets according to the accociated centroid. Each experiments were repeated 10 times to collect average test data log-likilihood and classification error.}

\hl{Table} \ref{tab:kernel} \hl{shows the quatitative numbers of performances and Figure} \ref{fig:kernel_increase_n} \hl{visualises the contuors of probability $p(y = 1|\bm{x}, \mathcal{D})$ with true posterior approximated by $q(\bm{\theta})$. Intestingly SEP is slightly better then the others on the classification error metric. But importantly EP achieves the best test log-likelihood numbers and in general DSEP produces very similar results (shown by both the table and the figure), meaning that even for small datasets running full EP might be unnecessary. Also the three methods become indistinguishable when the size of the dataset $N$ increases. We argue the main reason is that the posterior contributions are getting similar since more datapoints are observed in the circle of kernel width.}

\hl{We further tested the robustness of all three methods to outliers. We reused the settings above and randomly flipped $10\%$ labels of training data. Qualitative results in Figure} \ref{fig:kernel_flip} \hl{show that SEP is almost as robust as DSEP/EP in this example. We had tried different types of outliers and failed to find the cases where EP/DSEP significantly outperforms SEP. Future work should answer the questions that when SEP gives bad approximations and whether it fails in the same way as EP fails.}


\begin{table} 
\small
\centering
 \caption{ Average test results of all methods on kernel Probit regression.}
  \label{tab:kernel} 
\begin{tabular}{l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}
	l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}}\hline 
{} & \multicolumn{6}{c}{mean error} & \multicolumn{6}{c}{test log-likelihood} \\
\bf{$N$}&\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ DSEP }}&\multicolumn{2}{c}{\bf{ EP }} &\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ DSEP }}&\multicolumn{2}{c}{\bf{ EP }} \\ \hline 
%
$10*4$&\bf{0.032}&\bf{0.0058}&0.055&0.0127&0.032&0.0097 
	&-0.405&0.011&-0.380&0.010&\bf{-0.378}&\bf{0.009} \\
%
$20*4$&\bf{0.007}&\bf{0.0014}&0.008&0.0024&0.012&0.0031 
	&-0.326&0.007&-0.320&0.006&\bf{-0.317}&\bf{0.003} \\
%
$50*4$&\bf{0.003}&\bf{0.0010}&0.003&0.0014&0.006&0.0009 
	&-0.243&0.004&\bf{-0.233}&\bf{0.007}&-0.238&0.003 \\
%
 \hline \end{tabular} 
 \end{table} 

\begin{figure}
\centering
\def\svgwidth{1\linewidth}
\input{fig/increase_n_kernel.pdf_tex}
\caption{Comparing predictions of kernel Probit regression trained by SEP/DSEP/EP, with increasing training data size $N$.}
\label{fig:kernel_increase_n}
\end{figure}

\begin{figure}
\centering
\def\svgwidth{1\linewidth}
\input{fig/flip_kernel.pdf_tex}
\caption{Comparing predictions of kernel Probit regression trained by SEP/DSEP/EP, with $10\%$ labels flipped.}
\label{fig:kernel_flip}
\end{figure}

\newpage
\subsubsection*{References}
\renewcommand{\section}[2]{}
\small
\bibliographystyle{unsrt}
\bibliography{nips_sep}


\end{document}
