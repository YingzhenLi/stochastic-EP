\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{bm, subfigure}
\usepackage{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm, algorithmic}
\usepackage{graphicx}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\newtheorem{theorem}{Theorem}

\mathchardef\mhyphen="2D

\title{Stochastic Expectation Propagation: Supplementary Material}


\author{
Yingzhen Li \\
Department of Engineering\\
University of Cambridge\\
Cambridge, CB2 1PZ, UK \\
\texttt{yl494@cam.ac.uk} \\
\And
Richard E.~Turner \\
Affiliation \\
Address \\
\texttt{ret26@cam.ac.uk} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\appendix

\section{Further Theoretical Results}


\subsection{Equivalence between the ``Variational'' AEP and VI/VB}
We briefly discussed the equivalence between ``variational'' AEP and VI/VB.  
%
Without loss of generality we provide a proof on exponential families. We further assume that the prior $p_0(\bm{\theta})$ and the approximate posterior $q(\bm{\theta})$ are in the same family, but in general we might use different structures for approximate posterior and our proof still applies. 

We first define the exponential families for both likelihood terms and the approximate posterior,
\begin{align}
\mathcal{P} &= \{p(\bm{x} | \bm{\theta}) = \exp \left( \langle \beta(\bm{\theta}), \psi(\bm{x}) \rangle - A(\bm{\theta}) \right) |\ \bm{\theta} \in \bm{\Theta} \ s.t.~ A(\bm{\theta}) < \infty \}, \\
\mathcal{Q} &= \{q(\bm{\theta}) = \exp \left( \langle \bm{\lambda}_q, \phi(\bm{x}) \rangle - A(\bm{\lambda}_q) \right) |\ \bm{\lambda}_q \in \bm{\Lambda} \ s.t.~ A(\bm{\lambda}_q) < \infty \},
\end{align}
where $A(\cdot)$ represents the log partition function to guarantee a valid distribution. We denote the natural parameters of the prior $p_0(\bm{\theta})$, the approximating factor $f(\bm{\theta})$, and the intermediate answers $f_n(\bm{\theta})$ as $\bm{\lambda}_0$, $\bm{\lambda}$ and $\bm{\lambda}_n$, respectively. We next write the exponential family containing the true posterior and the tilted distribution:
\begin{equation}
\mathcal{\tilde{P}} = \{p(\bm{\theta}) = \exp \left( \langle \bm{\eta}, \Phi(\bm{\theta}) \rangle - A(\bm{\eta}) \right) |\ \bm{\eta} \in \bm{H} \ s.t.~ A(\bm{\eta}) < \infty \}, \\
\end{equation}
with sufficient statistics $\Phi(\bm{\theta}) = [\phi(\bm{\theta}), \beta(\bm{\theta}), A(\bm{\theta})]$ \footnote{We assume $\phi$, $\beta$ and $A$ are linearly independent wrt.~operation $E_{\tilde{p}}[\cdot], \forall \tilde{p} \in \tilde{\mathcal{P}}$ w.l.o.g.}. It is straight-forward that the true posterior has natural parameter $\bm{\eta}_p = [\bm{\lambda}_0, \sum_{n} \psi(\bm{x}_n), -N]$, and the tilted distributions $\tilde{p}_n(\bm{\theta})$ has $\bm{\eta}_n = [\bm{\lambda}_{-1}, \psi(\bm{x}_n), -1]$. We also write $\bm{\lambda}_q \leftarrow \mathtt{proj}[\bm{\eta}]$ to denote the moment projection of distributions in $\mathcal{\tilde{P}}$ to $\mathcal{Q}$.
%
We recall the AEP algorithm with current notations, using no-damped update,
%
\begin{align}
\bm{\lambda}_{-1} & = \bm{\lambda}_0 + \frac{N-1}{N} \sum_{n=1}^N \bm{\lambda}_n, \\
\bm{\lambda}_n &\leftarrow \mathtt{\alpha \mhyphen proj}[\bm{\eta}_n] - \bm{\lambda}_{-1}, \\
\bm{\lambda}_q &\leftarrow \bm{\lambda}_0 + \sum_{n=1}^N \bm{\lambda}_n.
\end{align}
%
The projection step is generalised to alpha-projection \cite{amari:alpha_proj}, which returns the minimiser of the alpha divergence $D_{\alpha}(\tilde{p}_n(\bm{\theta}) || q(\bm{\theta}))$ wrt.~$q(\bm{\theta})$ in $\mathcal{Q}$. Examples include the previous defined moment projection $\mathtt{proj}$ which take $\alpha = 1$, and information projection which choose $\alpha = -1$. 
%
Now we state the following theorem. 
%
\begin{theorem}
The ``variational'' averaged expectation propagation algorithm, when converged to a fixed point, returns a local optimum of variational inference.
\end{theorem}
\begin{proof}
Consider the same factorisation structure, $\bm{\lambda}_q = \bm{\lambda}_0 + N \bm{\lambda}$, for both VI and the ``variational'' AEP. VI directly optimises the global KL divergence wrt.~$q(\bm{\theta})$:
\begin{equation}
KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \langle \bm{\lambda}_q, E_q \phi(\bm{\theta}) \rangle - A(\bm{\lambda}_q) - \langle \bm{\eta}_p, E_q \Phi(\bm{\theta}) \rangle + A(\bm{\eta}_p).
\end{equation}
Using $\bm{\eta}_p = [\bm{\lambda}_0, \sum_{n} \psi(\bm{x}_n), -N]$,
\begin{equation}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \langle \bm{\lambda}_q - \bm{\lambda}_0, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \sum_{n} \psi(\bm{x}_n), \nabla_{\bm{\lambda}_q} E_q \beta(\bm{\theta}) \rangle + N \nabla_{\bm{\lambda}_q} E_q A(\bm{\theta}).
\end{equation}
On the other hand, the inner loop of the ``variational'' AEP finds the optimiser of the following divergence
\begin{equation}
KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})) = \langle \bm{\lambda}_q, E_q \phi(\bm{\theta}) \rangle - A(\bm{\lambda}_q) - \langle \bm{\eta}_n, E_q \Phi(\bm{\theta}) \rangle + A(\bm{\eta}_n).
\end{equation}
Now the local update we are interested in is $\bm{\lambda}_n = \bm{\lambda}_q - \bm{\lambda}_{-1}$, and we treat the cavity parameter $\bm{\lambda}_{-1}$ as a constant in this optimisation. Similarly we obtain the graident wrt.~$\bm{\lambda}_n$
\begin{equation}
\nabla_{\bm{\lambda}_n}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})) = \langle \bm{\lambda}_q - \bm{\lambda}_{-1}, \nabla_{\bm{\lambda}_q} E_q \phi(\bm{\theta}) \rangle - \langle \psi(\bm{x}_n), \nabla_{\bm{\lambda}_q} E_q \beta(\bm{\theta}) \rangle +  \nabla_{\bm{\lambda}_q} E_q A(\bm{\theta}).
\end{equation}
If we view the ``variational'' AEP as a gradient descent method, from $\bm{\lambda}_q \leftarrow \bm{\lambda}_0 + \sum_{n=1}^N \bm{\lambda}_n$ we have the gradient of the global parameter constructed by summing the gradients of the local parameters. Readers can verify that the gradient of the global parameter is equivalent to the gradient of VI objective, i.e.
\begin{equation}
\nabla_{\bm{\lambda}_q}^{VB} KL(q(\bm{\theta}) || p(\bm{\theta} | D)) = \sum_n \nabla_{\bm{\lambda}_n}^{AEP} KL(q(\bm{\theta}) || \tilde{p}_n(\bm{\theta})).
\end{equation}
This implies that the fixed points of ``variational'' AEP are also stationary points of VI.
\end{proof}

%
\subsection{SEP in a Global Optimisation Flavour}
We relate SEP to a global approximation algorithm though it is computed in a truely local way. This framework utilises alpha divergence but on the global posterior, and we motivate it by describing VI and SVI as divergence minimisation.
%
VI performs global optimisation on $KL(q(\bm{\theta})||p(\bm{\theta}|\mathcal{D}))$, and its stochastic version, SVI, is driven by the gradients of $KL(q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_n\}^N))$ on the $N$ replicates $\{\bm{x}_n\}^N$. Similarly, we state SEP as a stochastic global optimisation procedure, which minimises the $\alpha$-divergence $D_{\alpha}(p(\bm{\theta} | \{\bm{x}_n\}^N) || q_n(\bm{\theta}))$ with $\alpha = \mhyphen 1 + 2/N$. Indeed we can understand the inner-loop of AEP as PEP \cite{minka:powerep}, if considering the likelihood term of $\bm{x}_n$ raised to power $N$ as the only factor.

However directly minimising the alpha-divergence between the true posterior $p(\bm{\theta}|\mathcal{D})$ and the global approximation $q(\bm{\theta})$ recovers PEP on the whole dataset instead, and the factor to include in the tilted distribution changes to the intractable $\mathtt{avg}[\{p(\bm{x}_n | \bm{\theta}) \}]$. Readers might have noticed that the update of PEP on full dataset is given by $q(\bm{\theta}) \leftarrow \mathtt{proj}[\mathtt{avg}[\{ \tilde{p}_n(\bm{\theta}) \}]]$. In other words, we can interpret AEP as an approximation to the impractical batch PEP by interchanging computations, and we illustrate a geometric view for this in Figure \ref{fig:aep_vs_pep}.

It is important to note that SEP/AEP in convergence does not minimise the alpha divergence globally. Like PEP, the inner-loop compute alpha projections of $p(\bm{\theta} | \{\bm{x}_n\}^N)$, where one can show that it is equivalent to compute $\mathtt{proj}[\tilde{p}_n(\bm{\theta})]$ using techniques provided in \cite{minka:powerep}. The paper further proved that the stationary points of the projections of power EP on the whole dataset are equivalent to the fixed points of alpha divergence. However the outer-loop averages the natural parameters of the intermediate answers, which breaks the optimisation in the inner-loop. Indeed we provide the fixed points conditions of AEP which shows its local approximation nature.
%
\begin{theorem}
The fixed points of averaged EP, if exist, can be written as $q(\bm{\theta}) = \mathtt{avg}[\{q_n(\bm{\theta})\}]$, where
\begin{equation}
q_n(\bm{\theta}) = \mathtt{proj}[\tilde{p}_n(\bm{\theta})].
\label{eq:mm}
\end{equation}
These fixed points are also the fixed points of stochastic EP in expectation. 
\end{theorem}
\begin{proof}
In each update SEP gives the same answers as AEP in expectation if initialised at the same starting point. Also as an analogy to normal EP, the stationary points of AEP ask for moment matching between tilted and intermediate distributions. 
\end{proof}
%
SVI differs from SEP that in convergence it returns a local optimum of the global KL divergence $KL(q(\bm{\theta})|p(\mathcal{D}|\bm{\theta}))$. This is because, the variational KL divergence, or alpha divergence with $\alpha = -1$, is the only special case that has the same fixed points for local and global optimisation. 

\begin{figure}
\centering
\def\svgwidth{0.35\linewidth}
\subfigure[\label{fig:aep_vs_pep}]{
\input{fig/aep_vs_pep.pdf_tex}}
%
\caption{(a) A geometric view of AEP/PEP comparison}
\end{figure}

\section{Mathematical Details for EP updates}
\subsection{Mixture of Gaussians}

\subsection{Probit Regression}

\subsubsection*{References}
\renewcommand{\section}[2]{}
\bibliographystyle{apalike}
\bibliography{nips_sep}


\end{document}
