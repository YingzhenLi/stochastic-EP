We evaluate SEP on both synthetic and real-world data, and for brevity we omit the mathematical details. In synthetic tests we compare the approximations against the true posterior constructed by No-U-Turn sampler (NUTS) \cite{hoffman:nuts} implemented in \texttt{stan}\footnote{\url{http://mc-stan.org/pystan.html}}. We repeat all tests for 5 times.
%
For real datasets we test classification tasks with probit regression. We further conduct regression tasks using neural networks trained with probabilistic back-propagation \cite{miguel:pbp}, which is based on ADF in the first place. We modify the available code\footnote{\url{https://github.com/HIPS/Probabilistic-Backpropagation}} to support full EP and SEP training and compare their performances to the reported ADF results.

%
%%%% FIRST EXAMPLE %%%%
\subsection{Mixture of Gaussians for clustering}
We start from clustering using mixture of Gaussians. We construct the simulation model with $J=4$ Gaussians: $\bm{\mu}_j \sim \mathcal{N}(\bm{\mu}; \bm{m}, \Sigma)$, $\bm{y}_n \sim p_0(\bm{h}_n = j) \propto 1$, and $\bm{x}_n | \bm{y}_n \sim \mathcal{N}(\bm{x}_n; \bm{\mu}_{\bm{y}_n}, 0.5^2 I)$. We simulate 200 datapoints for clustering, and construct another Gaussian mixture model with uniform prior on $\bm{h}_n$ and independent prior $\mathcal{N}(\bm{\mu}_j; \bm{m}_j, 10I), \bm{m}_j \sim \mathcal{N}(\bm{m}; \bm{0}, I)$ on the Gaussian means. EP, SEP and ADF are applied to approximate the posterior of $\bm{\theta} = \{ \bm{\mu}_j \}$ with Gaussians and $\{\bm{h}_n\}$ with categorical distributions, though the storage for the latter terms is not required. 

Figure \ref{fig:gmm_visualised} visualises the estimated posterior and EP inference results after 200 iterations. The result strongly depends on the accurate estimation of posterior mode and thus ADF works well, however ADF actually converges towards a point estimation of some posterior mode. Instead, SEP captures the uncertainty and returns nearly identical approximations as full EP. We quantise the error of EP methods in Figure \ref{fig:gmm_error} by computing the averaged Frobenius norm of the difference between the the closest means (covariance) of true posterior samples and EP approximations. This further confirms that SEP approximates EP well.

\begin{figure}
\centering
\def\svgwidth{0.50\linewidth}
\subfigure[\label{fig:gmm_visualised}]{
\input{fig/gmm1.pdf_tex}}
%
\hspace{0.1in}
%
\def\svgwidth{0.40\linewidth}
\subfigure[\label{fig:gmm_error}]{
\input{fig/gmm_error.pdf_tex}}
\caption{Posterior approximation for the mean of the Gaussian components. (a) shows the estimated variance with sampling or EP methods in 98\% confidence level. The coloured dots indicate the true label (top-left) or the inferred cluster assignments (the rest). In (b) we show the quantitative error of EP approximations against the sampled ground truth, on mean (top) and covariance (bottom).}
\end{figure}

%%%% SECOND EXAMPLE %%%%
\subsection{Bayesian logistic regression}
We next consider Bayesian logistic regression with Probit function $P(\bm{y} = 1) = \Phi(\bm{\theta}^T \bm{x})$. 5,000 datapoints $\{ (\bm{x}_n, \bm{y}_n) \}$ with $D=4$ dimensional inputs are simulated from a pre-defined ground-truth model. The inputs $\bm{x}_n$ are sampled from a Gaussian distribution, and the binary labels $\bm{y}_n$ are regressed from a Probit unit with $\bm{\theta}_{true} \sim \mathcal{N}(\bm{\theta}; \bm{0}, I)$. For learning we use a Gaussian prior $\mathcal{N}(\bm{\theta}; \bm{0}, 1.5^2 I)$ and measure the performance by computing an approximate $KL(p(\bm{\theta}|\mathcal{D}) || q(\bm{\theta}))$ using the Gaussian constructed with sample mean and covariance as $p(\bm{\theta}|\mathcal{D})$.
%
Results in Figure \ref{fig:sep_probit} show that SEP is converging to the same local optimum as EP. Using SEP with larger size of minibatch is more robust with the price of slower learning. ADF collapses towards a delta function at the posterior mean and thus deviates from the true posterior. 

We also investigate the advantage of DSEP/DAEP when the observations are generated from very different clusters. We change the simulation model of $\bm{x}$ to a mixture of $J=5$ Gaussians, and partition the datasets into $K$ minibatches with datapoints from the same cluster. Figure \ref{fig:daep_probit} shows that SEP converges to slightly worse approximations as it only maintains the global posterior. In contrast DAEP performs nearly identical to full EP in convergence. The number of factors $K$ has little effect on the performance once $K \geq J$, indicating that the contributions of datapoints in the same cluster are very similar. 

To further verify the efficiency of DSEP on real dataset, we sample 1,000 datapoints for each digits from MNIST and perform odd-vs-even classification. We compare the performance of ADF, SEP, full EP and DSEP with $K=10$ factors (each one for each digit class) in Figure \ref{fig:mnist}. Now both EP and DSEP outperform ADF. DSEP is slightly worse than full EP in the first few iterations, however it reduces the memory to 0.001\% of full EP without losing to much accuracy.

We last test SEP's performance on real datasets without information of correct data partitions. We download 6 small datasets from the UCI machine learning repository\footnote{\url{https://archive.ics.uci.edu/ml/index.html}} and summarise classification results in Table \ref{tab:probit_results}. Similar to the previous tests that ADF learns the proper posterior mode and thus has the RMSE on predicted labels close to EP and SEP. However the flawed multiple passes return worse log-likelihood results, while EP models the uncertainty correctly and achieves significantly higher test log-likelihood than ADF. More importantly SEP produces very close performance as EP, implying that SEP can be an alternative of EP even for small data.

\begin{figure}
\centering
\def\svgwidth{0.31\linewidth}
\subfigure[\label{fig:sep_probit}]{
\input{fig/sep_probit.pdf_tex}}
%
%\hspace{0.01in}
%
\def\svgwidth{0.31\linewidth}
\subfigure[\label{fig:daep_probit}]{
\input{fig/daep.pdf_tex}}
%
%\hspace{0.01in}
%
\def\svgwidth{0.31\linewidth}
\subfigure[\label{fig:mnist}]{
\input{fig/mnist_error.pdf_tex}}
\caption{Performance of EP methods on Bayesian logistic regression.}
\end{figure}

\begin{table} 
\small
\centering \label{tab:probit_results} \begin{tabular}{l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}
	l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}}\hline 
{} & \multicolumn{6}{c}{RMSE} & \multicolumn{6}{c}{test log-likelihood} \\
\bf{Dataset}&\multicolumn{2}{c}{\bf{ ADF }}&\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ EP }} &\multicolumn{2}{c}{\bf{ ADF }}&\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ EP }} \\ \hline 
%
Australian&0.328&0.0127&\bf{0.325}&\bf{0.0135}&0.330&0.0133
	&-0.634&0.010&-0.631&0.009&\bf{-0.631}&\bf{0.009}\\
%
Breast&0.037&0.0045&\bf{0.034}&\bf{0.0034}&0.034&0.0039
	&-0.100&0.015&-0.094&0.011&\bf{-0.093}&\bf{0.011}\\
%
Crabs&0.062&0.0125&\bf{0.040}&\bf{0.0106}&0.048&0.0117
	&-0.290&0.010&\bf{-0.177}&\bf{0.012}&-0.217&0.011\\
%
Ionos&\bf{0.126}&\bf{0.0166}&0.130&0.0147&0.131&0.0149
	&-0.373&0.047&-0.336&0.029&\bf{-0.324}&\bf{0.028}\\
%
Pima&0.242&0.0093&0.244&0.0098&\bf{0.241}&\bf{0.0093}
	&-0.516&0.013&-0.514&0.012&\bf{-0.513}&\bf{0.012}\\
%
Sonar&\bf{0.198}&\bf{0.0208}&0.198&0.0217&0.198&0.0243
	&-0.461&0.053&-0.418&0.021&\bf{-0.415}&\bf{0.021}\\
 \hline \end{tabular} 
 \caption{ Average test results all methods on Probit regression. All methods capture a good posterior mean, however EP outperforms ADF in terms of test log-likelihood on almost all the datasets, with SEP very close to EP.}
 \end{table} 