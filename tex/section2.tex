

\section{Expectation Propagation and Assumed Density Filtering}
%short review on EP
We begin by briefly reviewing the EP and ADF algorithms upon which our new method is based. Consider for simplicity observing a dataset comprising $N$ i.i.d.~samples $\mathcal{D} = \{\bm{x}_n \}_{n=1}^N$ from a probabilistic model $p(\bm{x}|\bm{\theta})$ parametrised by an unknown $K$-dimensional vector $\bm{\theta}$ that is drawn from a prior $p_0(\bm{\theta})$. Exact Bayesian inference involves computing the posterior distribution of the parameters given the data, 
\begin{equation}
p(\bm{\theta} | \mathcal{D}) \propto p_0(\bm{\theta}) \prod_{n=1}^{N} p(\bm{x}_n | \bm{\theta}).
\end{equation}
%
Typically, the posterior is intractable and requires approximation. A standard application of EP constructs such an approximation using a simpler distribution $q(\bm{\theta}) \in \mathcal{Q}$ comprising a product of simple factors,
%
% and from a distribution family $\mathcal{P}$, is introduced to estimate the underlying data distribution. Bayesian methods require posterior computation after observing dataset $D = \{\bm{x}_i \}_{i=1}^N$ using Bayes Rule: 
%\begin{equation}
%p(\bm{\theta} | D) \propto p_0(\bm{\theta}) \prod_{i=1}^{N} p(\bm{x}_i | \bm{\theta}),
%\end{equation}
%however this posterior is often in some intractable family $\tilde{\mathcal{P}}$ for many powerful probabilistic models. Expectation propagation approximates the true posterior with a suitable distribution $q(\bm{\theta}) \in \mathcal{Q}$, which factorises over likelihood terms
%
\begin{equation}
q(\bm{\theta}) \propto p_0(\bm{\theta}) \prod_{n=1}^{N} f_n(\bm{\theta}).
\end{equation}
%
The objective is to refine the approximate factors so that they capture the contribution of each of the likelihood terms to the posterior i.e.~$f_n(\bm{\theta}) \approx p(\bm{x}_n | \bm{\theta})$. In this spirit, one approach would be to find each approximating factor $f_n(\bm{\theta})$ by minimising the Kullback Leibler (KL) Divergence between the posterior and the distribution formed by replacing one of the likelihoods by its corresponding approximating factor,  $\mathrm{KL}[p(\bm{\theta}|\mathcal{D}) || p(\bm{\theta}|\mathcal{D}) f_n(\bm{\theta})/ p(\bm{x}_n | \bm{\theta})]$. Unfortunately, such an update is still intractable as it involves computing the full posterior. Instead, EP approximates this procedure by replacing the exact leave-one-out posterior $p_{-n}(\bm{\theta}) = p(\bm{\theta}|\mathcal{D}) / p(\bm{x}_n | \bm{\theta})$ on both sides of the KL by the approximate leave-one-out posterior (called the cavity distribution) $q_{-n}(\bm{\theta}) =q(\bm{\theta})/f_n(\bm{\theta})$. Since this couples the updates for the approximating factors, the updates must now be iterated.

In more detail, EP iterates four simple steps. First, the factor selected for update is removed from the approximation to produce the cavity distribution. Second, the corresponding likelihood is included to produce the tilted distribution $\tilde{p}_n(\bm{\theta}) = q_{-n}(\bm{\theta}) p(\bm{x}_n | \bm{\theta})$. Third EP updates the approximating factor by minimising $\mathrm{KL}[\tilde{p}_n(\bm{\theta}) || q_{-n}(\bm{\theta})  f_n(\bm{\theta})]$. The hope is that the contribution the true-likelihood makes to the posterior is similar to the effect the same likelihood has on the tilted distribution. If the approximating distribution is in the exponential family, as is often the case, then the KL minimisation reduces to a moment-matching step \cite{amari:ig} that we denote $f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $. Finally, having updated the factor, it is included into the approximating distribution.\todo[fancyline]{mention damping here?}
%
%Next EP updates the  $\mathrm{KL}(p(\bm{\theta})||q(\bm{\theta}))$
%
% is removed from the approximating distribution
%
%
%by matching the moments of the corresponded single datapoint posterior. To be precise, an EP iteration begins with removing the selected factor $f_i(\bm{\theta})$ to form the cavity distribution $q_{-i}(\bm{\theta})$, then uses it as the prior distribution to incorporate the current likelihood $p(\bm{x}_i| \bm{\theta})$. The local posterior $\tilde{p}_i(\bm{\theta}) \propto p(\bm{x}_i|\bm{\theta}) q_{-i}(\bm{\theta})$ is also referred as the tilted distribution, which is in the $\tilde{\mathcal{P}}$ family as well. 
%
%Next EP proposes a moment projection (M-projection) \cite{amari:ig} or moment matching step to approximate the tilted distribution by minimising $KL(p(\bm{\theta})||q(\bm{\theta}))$ wrt.~$q(\bm{\theta})$, and finally recovers the new updates of the current factor $f_i(\bm{\theta}) \buildrel\propto\over \leftarrow q(\bm{\theta}) / q_{-1}(\bm{\theta})$.

We summarise the update procedure for a single factor in Algorithm \ref{alg:ep}. Critically, the approximation step of EP involves local computations since one likelihood term is treated at a time. The assumption is that these local computations, although possibly requiring further approximation, are far simpler to handle compared to the full posterior $p(\bm{\theta}| \mathcal{D})$. In practice, EP often performs well when the updates are parallelised. Moreover, by using approximating factors for groups of data-points, and then running additional approximate inference algorithms to perform the EP updates (which could include nesting EP), EP carves up the dataset meaning it is a good candidate for distributed approximate inference.\todo[fancyline]{Mention full factor graph interpretation, message passing? theory available for EP here? fixed point properties, marginal likelihood?}

% ADF
There is, however, one wrinkle that complicates deployment of EP at scale. Computation of the cavity distribution requires removal of the current approximating factor and this means that any implementation of EP must store them explicitly necessitating an $\mathcal{O}(N)$ memory footprint. One option is to simply ignore the removal step replacing the cavity distribution with the full approximation resulting in the ADF algorithm (see Algorithm \ref{alg:adf}). ADF has the advantage that only the global approximation need be maintained in memory, but as the moment matching step now over-counts the underlying approximating factor (consider the new form of the objective $\mathrm{KL}[q(\mathbf{\theta}) p(\bm{x}_n | \bm{\theta}) || q(\bm{\theta})]$) the variance of the approximation shrinks to zero as multiple passes are made through the dataset. Early stopping is therefore required to prevent overfitting and generally speaking ADF does not uncertainties that are well-calibrated to the posterior. 

In the next section we introduce a new algorithm that sidesteps EP's large memory demands whilst avoiding the pathological behaviour of ADF. 

\section{Stochastic Expectation Propagation}
%
% SEP
In this section we introduce a new algorithm, inspired by EP, called SEP that combines the benefits of local approximation (including tractability of updates, distributability, and parallelisability) with global approximation (reduced memory demands).  The algorithm can be interpreted as a version of EP in which the approximating factors are tied, or alternatively as a corrected version of ADF that prevents overfitting. 

The key idea is that, at convergence, the approximating factors in EP can be interpreted as parameterising a global factor,  $f(\bm{\theta})$, that captures the average effect of a likelihood on the posterior  $\prod_{n=1}^{N} f_n(\bm{\theta}) = f(\bm{\theta})^{N} \approx \prod_{n=1}^{N} p(\bm{x}_n | \bm{\theta})$. In this spirit, the new algorithm employs direct iterative refinement of a global approximation comprising the prior and $N$ copies of a single approximating factor, $f(\bm{\theta})$, %
%EP has been shown very successful in previous investigations as mentioned, however very little work has been done on large datasets due to its large memory consumption. It requires the program to store every local approximator $f_i(\bm{\theta})$, resulting in space complexity $\mathcal{O}(Nd^2)$ if using Gaussians. To eliminate the linear factor $N$ in the storage requirement, we propose a factor-tying approach by defining a new approximation structure
%
\begin{equation}
q(\bm{\theta}) \propto f(\bm{\theta})^N p_0(\bm{\theta}).
\end{equation}
%
%The goal is to refine $f(\bm{\theta})$ in such a way that it captures the average effect a likelihood function has on the posterior. 
SEP uses updates that are analogous to EP's in order to refine $f(\bm{\theta})$ in such a way that it captures the average effect a likelihood function has on the posterior. First the cavity distribution is formed by removing one of the copies of the factor, $q_{-n}(\bm{\theta}) =q(\bm{\theta})/f(\bm{\theta})$. 
Second, the corresponding likelihood is included to produce the tilted distribution $\tilde{p}_n(\bm{\theta}) = q_{-1}(\bm{\theta}) p(\bm{x}_n | \bm{\theta})$ and, third, EP finds an intermediate factor approximation by moment matching, $f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta}) $. Finally, having updated the factor, it is included into the approximating distribution. It is important here not to make a full update since $f_n(\bm{\theta})$ captures the effect of just a single likelihood function  $p(\bm{x}_n | \bm{\theta})$. Instead, damping should be employed to make a partial update $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \alpha} f_n(\bm{\theta})^{\alpha}$ a natural choice uses $\alpha = 1/N$ which can be interpreted as minimising  $\mathrm{KL}[\tilde{p}_n(\bm{\theta}) || p_{0}(\theta)  f(\bm{\theta})^N]$ in the moment update.

SEP is summaried in Algorithm \ref{alg:sep}. Unlike ADF, the cavity is formed by dividing out $f(\bm{\theta})$ which captures the average affect of the likelihood and prevents the posterior from collapsing. Like ADF, however, memory allocation for $f(\bm{\theta})$ is unnecessary because it can be recovered from the approximate posterior, $f(\bm{\theta}) \propto (q(\bm{\theta}) / p_0(\bm{\theta}))^{\frac{1}{N}}$ and $q_{-1}(\bm{\theta}) \propto q(\bm{\theta})^{1 - \frac{1}{N}} p_0(\bm{\theta})^{\frac{1}{N}}$. When Gaussian approximating factors are used, for example, SEP reduces the storage requirement of EP from  $\mathcal{O}(NK^2)$ to $\mathcal{O}(K^2)$ which is a substantial saving that enables models with many parameters to be applied to large datasets. 

\begin{figure}[!t]
% UGLY USE OF \vspace & \hspace follows
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{EP} \small
\label{alg:ep} 
\begin{algorithmic}[1] 
	\STATE choose a factor $f_n$ to refine:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) \propto q(\bm{\theta}) / f_n(\bm{\theta})$ 
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-n}(\bm{\theta}) f_n(\bm{\theta})$\\\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{ADF} \small
\label{alg:adf} 
\begin{algorithmic}[1] 
	\STATE choose a datapoint $\bm{x}_n\sim D$:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) = q(\bm{\theta})$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-n}(\bm{\theta}) f_n(\bm{\theta})$\\\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%\quad
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H]
\caption{SEP} \small
\label{alg:sep} 
\begin{algorithmic}[1] 
%\STATE initialize $\{\tilde{f}_a\}$
	\STATE choose a datapoint $\bm{x}_n\sim D$:
	\STATE compute cavity distribution \\ $q_{-1}(\bm{\theta}) \propto q(\bm{\theta}) / f(\bm{\theta})$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-1}(\bm{\theta})$
	\STATE moment matching: \\\hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-1}(\bm{\theta}) f_n(\bm{\theta})$
	\STATE \textit{implicit update}:\\ $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \frac{1}{N}} f_n(\bm{\theta})^{\frac{1}{N}}$
\end{algorithmic}
\end{algorithm}
\end{minipage} 
%
\caption{Comparing the Expectation Propagation (EP), Assumed Density Filtering (ADF), and Stochastic Expectation Propagation (SEP) update steps. Typically, the algorithms will be initialised using $q(\mathbf{\theta}) = p_0(\mathbf{\theta})$ and, where appropriate, $f_n(\mathbf{\theta})=1$ or $f(\mathbf{\theta})=1$.}
\end{figure}

%
%We also consider Assumed density filtering (ADF) \cite{maybeck:adf}\cite{minka:ep}, the streaming version of EP, and show its connection to SEP. ADF successively incorporates the %incoming datapoints to the approximate posterior by using the posterior computed on previous samples as the current prior. Hence the algorithm only stores the global posterior and is scalable on large datasets. However it always treats the next input as a new observation, making ADF with multiple passes of data flawed in nature. 
%
%We re-introduce SEP as to correct ADF by re-scaling the parameters (line 2 in Algorithm \ref{alg:sep}). In this way SEP preserves the advantage of ADF in memory consumption but also retains the correct uncertainty level as full EP. Also multiple passes of datasets helps $q(\bm{\theta})$ ``forget" the bad approximations gradually, making SEP much more robust to observation ordering. 