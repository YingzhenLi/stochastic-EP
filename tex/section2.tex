

\section{Expectation Propagation and Assumed Density Filtering}
%short review on EP
We begin by briefly reviewing the EP and ADF algorithms upon which our new method is based. Consider for simplicity observing a dataset comprising $N$ i.i.d.~samples $\mathcal{D} = \{\bm{x}_n \}_{n=1}^N$ from a probabilistic model $p(\bm{x}|\bm{\theta})$ parametrised by an unknown $D$-dimensional vector $\bm{\theta}$ that is drawn from a prior $p_0(\bm{\theta})$. Exact Bayesian inference involves computing the (typically intractable) posterior distribution of the parameters given the data, 
\begin{equation}
p(\bm{\theta} | \mathcal{D}) \propto p_0(\bm{\theta}) \prod_{n=1}^{N} p(\bm{x}_n | \bm{\theta}) \approx q(\bm{\theta}) \propto p_0(\bm{\theta}) \prod_{n=1}^{N} f_n(\bm{\theta}).
\end{equation}
%
Here $q(\bm{\theta})$  is a simpler tractable approximating distribution that will be refined by EP.
%
The goal of EP is to refine the approximate factors so that they capture the contribution of each of the likelihood terms to the posterior i.e.~$f_n(\bm{\theta}) \approx p(\bm{x}_n | \bm{\theta})$. In this spirit, one approach would be to find each approximating factor $f_n(\bm{\theta})$ by minimising the Kullback Leibler (KL) Divergence between the posterior and the distribution formed by replacing one of the likelihoods by its corresponding approximating factor,  $\mathrm{KL}[p(\bm{\theta}|\mathcal{D}) || p(\bm{\theta}|\mathcal{D}) f_n(\bm{\theta})/ p(\bm{x}_n | \bm{\theta})]$. Unfortunately, such an update is still intractable as it involves computing the full posterior. Instead, EP approximates this procedure by replacing the exact leave-one-out posterior $p_{-n}(\bm{\theta}) \propto p(\bm{\theta}|\mathcal{D}) / p(\bm{x}_n | \bm{\theta})$ on both sides of the KL by the approximate leave-one-out posterior (called the cavity distribution) $q_{-n}(\bm{\theta}) \propto q(\bm{\theta})/f_n(\bm{\theta})$. Since this couples the updates for the approximating factors, the updates must now be iterated.

In more detail, EP iterates four simple steps. First, the factor selected for update is removed from the approximation to produce the cavity distribution. Second, the corresponding likelihood is included to produce the tilted distribution $\tilde{p}_n(\bm{\theta}) \propto q_{-n}(\bm{\theta}) p(\bm{x}_n | \bm{\theta})$. Third EP updates the approximating factor by minimising $\mathrm{KL}[\tilde{p}_n(\bm{\theta}) || q_{-n}(\bm{\theta})  f_n(\bm{\theta})]$. The hope is that the contribution the true-likelihood makes to the posterior is similar to the effect the same likelihood has on the tilted distribution. If the approximating distribution is in the exponential family, as is often the case, then the KL minimisation reduces to a moment-matching step \cite{amari:ig} that we denote $f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $. Finally, having updated the factor, it is included into the approximating distribution.
%

We summarise the update procedure for a single factor in Algorithm \ref{alg:ep}. Critically, the approximation step of EP involves local computations since one likelihood term is treated at a time. The assumption is that these local computations, although possibly requiring further approximation, are far simpler to handle compared to the full posterior $p(\bm{\theta}| \mathcal{D})$. In practice, EP often performs well when the updates are parallelised. Moreover, by using approximating factors for groups of data-points, and then running additional approximate inference algorithms to perform the EP updates (which could include nesting EP), EP carves up the data making it suitable for distributed approximate inference.

\begin{figure}[!t]
% UGLY USE OF \vspace & \hspace follows
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{EP} \small
\label{alg:ep} 
\begin{algorithmic}[1] 
	\STATE choose a factor $f_n$ to refine:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) \propto q(\bm{\theta}) / f_n(\bm{\theta})$ 
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-n}(\bm{\theta}) f_n(\bm{\theta})$\\\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H] 
\caption{ADF} \small
\label{alg:adf} 
\begin{algorithmic}[1] 
	\STATE choose a datapoint $\bm{x}_n\sim \mathcal{D}$:
	\STATE compute cavity distribution \\$q_{-n}(\bm{\theta}) = q(\bm{\theta})$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-n}(\bm{\theta})$
	\STATE moment matching: \\ \hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-n}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-n}(\bm{\theta}) f_n(\bm{\theta})$\\\hspace{1mm}\\ \vspace{1.5mm} \hspace{1mm}\\
\end{algorithmic}
\end{algorithm}
\end{minipage}
%\quad
\begin{minipage}[t]{0.33\linewidth}
\centering
\begin{algorithm}[H]
\caption{SEP} \small
\label{alg:sep} 
\begin{algorithmic}[1] 
%\STATE initialize $\{\tilde{f}_a\}$
	\STATE choose a datapoint $\bm{x}_n\sim \mathcal{D}$:
	\STATE compute cavity distribution \\ $q_{-1}(\bm{\theta}) \propto q(\bm{\theta}) / f(\bm{\theta})$
	\STATE compute tilted distribution \\$\tilde{p}_n(\bm{\theta}) \propto p(\bm{x}_n|\bm{\theta}) q_{-1}(\bm{\theta})$
	\STATE moment matching: \\\hspace{-1mm}$f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta}) $
	\STATE inclusion:\\ $q(\bm{\theta}) \leftarrow q_{-1}(\bm{\theta}) f_n(\bm{\theta})$
	\STATE \textit{implicit update}:\\ $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \frac{1}{N}} f_n(\bm{\theta})^{\frac{1}{N}}$
\end{algorithmic}
\end{algorithm}
\end{minipage} 
%
\caption{Comparing the Expectation Propagation (EP), Assumed Density Filtering (ADF), and Stochastic Expectation Propagation (SEP) update steps. Typically, the algorithms will be initialised using $q(\bm{\theta}) = p_0(\bm{\theta})$ and, where appropriate, $f_n(\bm{\theta})=1$ or $f(\bm{\theta})=1$.}
\end{figure}

% ADF
There is, however, one wrinkle that complicates deployment of EP at scale. Computation of the cavity distribution requires removal of the current approximating factor, which means any implementation of EP must store them explicitly necessitating an $\mathcal{O}(N)$ memory footprint. One option is to simply ignore the removal step replacing the cavity distribution with the full approximation, resulting in the ADF algorithm (Algorithm \ref{alg:adf}) that only maintains global approximation in memory. But as the moment matching step now over-counts the underlying approximating factor (consider the new form of the objective $\mathrm{KL}[q(\bm{\theta}) p(\bm{x}_n | \bm{\theta}) || q(\bm{\theta})]$) the variance of the approximation shrinks to zero as multiple passes are made through the dataset. Early stopping is therefore required to prevent overfitting and generally speaking ADF does not return uncertainties that are well-calibrated to the posterior. 
%
In the next section we introduce a new algorithm that sidesteps EP's large memory demands whilst avoiding the pathological behaviour of ADF. 

\section{Stochastic Expectation Propagation}
%
% SEP
In this section we introduce a new algorithm, inspired by EP, called Stochastic Expectation Propagation (SEP) that combines the benefits of local approximation (tractability of updates, distributability, and parallelisability) with global approximation (reduced memory demands).  The algorithm can be interpreted as a version of EP in which the approximating factors are tied, or alternatively as a corrected version of ADF that prevents overfitting. 
%
The key idea is that, at convergence, the approximating factors in EP can be interpreted as parameterising a global factor,  $f(\bm{\theta})$, that captures the average effect of a likelihood on the posterior  $f(\bm{\theta})^{N} \buildrel\triangle\over = \prod_{n=1}^{N} f_n(\bm{\theta}) \approx \prod_{n=1}^{N} p(\bm{x}_n | \bm{\theta})$. In this spirit, the new algorithm employs direct iterative refinement of a global approximation comprising the prior and $N$ copies of a single approximating factor, $f(\bm{\theta})$, that is $q(\bm{\theta}) \propto f(\bm{\theta})^N p_0(\bm{\theta})$.

SEP uses updates that are analogous to EP's in order to refine $f(\bm{\theta})$ in such a way that it captures the average effect a likelihood function has on the posterior. First the cavity distribution is formed by removing one of the copies of the factor, $q_{-1}(\bm{\theta}) \propto q(\bm{\theta})/f(\bm{\theta})$. 
Second, the corresponding likelihood is included to produce the tilted distribution $\tilde{p}_n(\bm{\theta}) \propto q_{-1}(\bm{\theta}) p(\bm{x}_n | \bm{\theta})$ and, third, SEP finds an intermediate factor approximation by moment matching, $f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta}) $. Finally, having updated the factor, it is included into the approximating distribution. It is important here not to make a full update since $f_n(\bm{\theta})$ captures the effect of just a single likelihood function  $p(\bm{x}_n | \bm{\theta})$. Instead, damping should be employed to make a partial update $f(\bm{\theta}) \leftarrow f(\bm{\theta})^{1 - \epsilon} f_n(\bm{\theta})^{\epsilon}$ a natural choice uses $\epsilon = 1/N$ which can be interpreted as minimising  $\mathrm{KL}[\tilde{p}_n(\bm{\theta}) || p_{0}(\bm{\theta})  f(\bm{\theta})^N]$ in the moment update.

SEP is summarised in Algorithm \ref{alg:sep}. Unlike ADF, the cavity is formed by dividing out $f(\bm{\theta})$ which captures the average affect of the likelihood and prevents the posterior from collapsing. Like ADF, however, SEP only maintains the global approximation $q(\bm{\theta})$ since $f(\bm{\theta}) \propto (q(\bm{\theta}) / p_0(\bm{\theta}))^{\frac{1}{N}}$ and $q_{-1}(\bm{\theta}) \propto q(\bm{\theta})^{1 - \frac{1}{N}} p_0(\bm{\theta})^{\frac{1}{N}}$. When Gaussian approximating factors are used, for example, SEP reduces the storage requirement of EP from  $\mathcal{O}(ND^2)$ to $\mathcal{O}(D^2)$ which is a substantial saving that enables models with many parameters to be applied to large datasets. 


