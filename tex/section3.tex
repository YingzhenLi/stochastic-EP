\section{Understanding stochastic expectation propagation}
In this section we provide theoretical understandings of stochastic EP, though the algorithm itself is motivated by practical computation issues. 

\subsection{Considering expectations of stochastic EP updates}
One important analysis of a stochastic algorithm is its dynamics in expectation, and the expectation of SEP's update step is the geometric average of new factors
$\bar{f}(\bm{\theta}) \propto [\prod_{i=1}^N f_i(\bm{\theta})]^{\frac{1}{N}}$.
This motivates the averaged EP (AEP) algorithm as the expectation version of stochastic EP, which performs almost the same computations, except that in the moment matching step it collects all the intermediate approximations $f_i(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_i(\bm{\theta})] / q_{-1}(\bm{\theta})$ and use $\bar{f}(\bm{\theta})$ in the next inclusion step. 
%%%%
Unfortunately, finding out the loss function/free energy of AEP is not a trivial task. Instead we introduce a set of auxiliary distributions $\{q_i(\bm{\theta})\}$ and frame AEP as a double-loop algorithm. After the cavity computation step, the update $q(\bm{\theta})$ of AEP is constructed as follows:
%
\begin{itemize}
	\item \textbf{inner loop:} $q_i(\bm{\theta}) \leftarrow \arg\min_q KL(\tilde{p}_i(\bm{\theta}) ||q(\bm{\theta})) \buildrel\triangle\over = \mathtt{proj}[\tilde{p}_i(\bm{\theta})]$; 
	\item \textbf{outer loop:} $q(\bm{\theta}) \leftarrow \arg\min_q \sum_{i=1}^N KL(q(\bm{\theta}) ||q_i(\bm{\theta})) \buildrel\triangle\over = \mathtt{avg}[\{q_i(\bm{\theta})\}]$.
\end{itemize}
%
Note that the operator $\mathtt{avg}[\cdot]: \mathcal{Q}^N \rightarrow \mathcal{Q}$ can be defined on other distribution families similarly, and we abbreviate the formal definitions when re-introducing it later.
This framework allows us to analyse the convergence behaviour of AEP, where we have the following theorem.
%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
The fixed points of averaged EP, if exist, can be written as $q(\bm{\theta}) = \mathtt{avg}[\{q_i(\bm{\theta})\}]$, where
\begin{equation}
q_i(\bm{\theta}) = \mathtt{proj}[\tilde{p}_i(\bm{\theta})].
\label{eq:mm}
\end{equation}
These fixed points are also the fixed points of stochastic EP in expectation. 
\end{theorem}
\begin{proof}
In each update SEP gives the same answers as AEP in expectation if initialised at the same starting point. Also as an analogy to normal EP, the stationary points of AEP ask for moment matching between tilted and auxiliary distributions. 
\end{proof}
\textbf{Remark.} The convergence condition (\ref{eq:mm}) differs from the moment matching condition of full EP in that the matching requirements are proposed on the auxiliary distributions $\{q_i(\bm{\theta}) \}$ rather than the global approximation $q(\bm{\theta})$. The constraints are implicitly contained in the shared cavity distribution.

%
%
Averaged EP has also been proposed in \cite{barthelme:aep} but as a theoretical tool to study the convergence properties of normal EP. The authors showed that averaged EP can be interpreted as Newton methods in large data limit and proved the convergence chain EP $\rightarrow$ AEP $\rightarrow$ Newton in that case. However the results are very restrictive as it requires log-concave and very slow changing likelihood functions. 

\subsection{Connections to stochastic variational inference}
Applying stochastic optimisation to EP is also motivated by the great success of stochastic variational inference (SVI) \cite{hoffman:svi}. Both require memory allocation for global posterior only, compute updates based on the current sample, and perform updates with a suitable damping schedule. However SEP differs from SVI in the first view: the former processes local approximations, while the latter performs global optimisation. We attempt to connect SEP with SVI in a general framework, and we sketch two possible directions as local or global computation.

%
\textbf{Connecting SVI to a local optimisation algorithm.}
To understand SVI from a local optimisation perspective, consider VB as the expectation of SVI. We introduce a conceptual ``variational'' AEP by changing the moment matching step in the inner-loop to minimising the variational KL-divergence $KL(q_i(\bm{\theta}) || \tilde{p}_i(\bm{\theta}))$ wrt.~$q_i(\bm{\theta})$ \footnote{Also called information projection (I-projection) in \cite{amari:ig}).}. A similar version of EP is also presented in \cite{minka:divergence} as a special case of a generic message passing algorithm, where the author showed that EP with variational KL is equivalent to the variational message passing algorithm \cite{winn:vmp}. This result also applies to the ``variational'' AEP as the outer-loop computes a geometric average, with a formal proof in the supplementary material. 

%
\textbf{SEP in a global optimisation flavour.}
VB performs global optimisation on $KL(q(\bm{\theta})||p(\bm{\theta}|D))$, and its stochastic version, SVI, is driven by the gradients of $KL(q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_i\}^N))$ on the $N$ replicates $\{\bm{x}_i\}^N$. Similarly, SEP computes a noisy estimation $q_i(\bm{\theta})$ by running the AEP inner-loop for the current sample only, though it is still a local optimisation at the first glance. However SEP can also be presented in a power EP (PEP) \cite{minka:powerep} fashion, if considering the likelihood term of $\bm{x}_i$ raised to power $N$ as the only factor. This allows us to state SEP as a stochastic global optimisation procedure, which minimises the $\alpha$-divergence \footnote{$D_{1}(p||q) \buildrel\triangle\over = KL(p || q) = \lim_{\alpha \rightarrow 1} D_{\alpha}(p||q)$, and $D_{ \mhyphen 1}(p||q) \buildrel\triangle\over = KL(q || p) = \lim_{\alpha \rightarrow \mhyphen 1} D_{\alpha}(p || q)$. } \cite{amari:ig1985}
\begin{equation}
D_{\alpha}(p(\bm{\theta} | \{\bm{x}_i\}^N) || q_i(\bm{\theta})) = \frac{4}{1 - \alpha^2} 
		\left( 1 - \int_{\bm{\theta}} p(\bm{\theta} | \{\bm{x}_i\}^N)^{(1 + \alpha) / 2} q_i(\bm{\theta})^{(1 - \alpha) / 2} d\bm{\theta} \right)
\end{equation} 
with $\alpha = \mhyphen 1 + 2/N$. Until now we managed to describe SVI and SEP in the same stochastic optimisation framework, which moves towards minimising $D(p(\bm{\theta} | \{\bm{x}_i\}^N), q(\bm{\theta}))$ for a given distance/divergence measure $D(\cdot, \cdot)$ on distributions. 

%
However applying this framework to the true posterior $p(\bm{\theta}|D)$ is often inconsistent with the expectation of stochastic optimisation. For the $\alpha$-divergence we selected, it recovers PEP on the whole dataset instead, and the factor to include in the tilted distribution changes to the intractable $\mathtt{avg}[\{p(\bm{x}_i | \bm{\theta}) \}]$. Readers might have noticed that the update of PEP on full dataset is given by $q(\bm{\theta}) \leftarrow \mathtt{proj}[\mathtt{avg}[\{ \tilde{p}_i(\bm{\theta}) \}]]$. In other words, we can restate the batch PEP algorithm by switching the order of M-projections and geometric averaging in AEP. Conversely we can interpret AEP as an approximation to the impractical batch PEP by interchanging computations. Figure \ref{fig:aep_vs_pep} illustrate a geometric view of this approximation, where the bias of this order swap is non-zero for most $D(\cdot, \cdot)$.

\subsection{Large-data limit revisited}
We provide a comparison between SEP and SVI in large-data limit using the global optimisation framework. We denote $\alpha \mhyphen \mathtt{proj}[p(\bm{\theta})]$ as the $\alpha$-projection \cite{amari:ig1985} \cite{amari:ig} of any distribution $p(\bm{\theta})$ to the $\mathcal{Q}$ family. Using this notation the previously defined $\mathtt{proj}[\cdot]$ operator is the $1$-projection, while SVI takes $\alpha = \mhyphen 1$. A similar analysis as \cite{amari:alpha_proj} indicates that in the large-data limit ($N \rightarrow \infty$), the $\alpha$-projection step obtains a minimiser of the variational $KL(q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_i\}^N))$. However it does not necessary imply the equivalence between SEP and SVI since both algorithms in the inner-loop do not directly minimise their divergence objective. Instead they iteratively compute the current stochastic estimate $q_i(\bm{\theta})$ based on the previous solution, e.g.~line 4 in SEP Algorithm \ref{alg:sep}, and more importantly that iterative process is not continuous wrt.~$\alpha$ at $\alpha = -1$. Aslo the inner-loop divergences are very unlikely to reach their minimum as the outer loop violates the gradient descent computations. Further, even all of the auxiliary distributions converges to the local optima of the variational KL-divergences, there is no guarantee that the global approximation after averaging converges to a VB local optimum. A better restatement of SEP/AEP with infinite observations would be that SEP tends to behave like SVI when $N$ goes to infinity, although the connection of fixed point behaviour is still unclear. 

%
\begin{figure}
\centering
\def\svgwidth{0.35\linewidth}
\subfigure[\label{fig:aep_vs_pep}]{
\input{fig/aep_vs_pep.pdf_tex}}
%
\hspace{0.5in}
%
\def\svgwidth{0.4\linewidth}
\subfigure[\label{fig:dep_sep_dsep}]{
\input{fig/dep_sep_dsep.pdf_tex}}

\caption{(a) A geometric view of AEP/PEP comparison. (b) A cartoon illustration of DEP, SEP and DSEP. For each algorithm we show the approximate posterior on the top and the tilted distribution at the bottom.}

\end{figure}