\section{Understanding stochastic expectation propagation}
%
SEP has been motivated from practical considerations, but here we develop a small number of theoretical justifications for the approach. First we show that, under certain conditions, the fixed points of EP are the same as the mean of the SEP steady-state. Second, we show a similar result for a more general version of SEP that handles mini-batch updates. Third, we show that SEP relates to SVI in an analogous way that EP relates to VI.

%
\subsection{Relating the EP fixed points to SEP}

Since SEP is a stochastic algorithm, we first analyse its dynamics in expectation.  SEP's approximating factor $f(\bm{\theta})$ converges, in expectation, to the geometric average of the intermediate factors $\bar{f}(\bm{\theta}) \propto [\prod_{n=1}^N f_n(\bm{\theta})]^{\frac{1}{N}}$. 


Notice then, that if SEP is parallelised in precisely the way EP is parallelised -- so that $N$ intermediate factors are simultaneously computed for each likelihood \footnote{Notice that a) unlike EP the cavity will be the same for all updates, and b) } $f_n(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_n(\bm{\theta})] / q_{-1}(\bm{\theta})$ and the approximating factors $f(\bm{\theta})$ are updated using the expression for $\bar{f}(\bm{\theta}$) -- the dynamics will converge to the same point. This algorithm is known as Averaged EP (AEP) and it was originally proposed in \cite{barthelme:aep} as a theoretical tool to study the convergence properties of normal EP. This work showed that, under fairly restrictive conditions (log-concave and very slow changing likelihood functions\todo[fancyline]{YL: what does slowly varying mean in this context?}), AEP converges to the same fixed points as EP.

%The authors showed that averaged EP can be interpreted as Newton methods in large data limit and proved the convergence chain EP $\rightarrow$ AEP $\rightarrow$ Newton in that case. %However the results are very restrictive as it requires log-concave and very slow changing likelihood functions. 




%This motivates the averaged EP (AEP) algorithm as the expectation version of stochastic EP, which performs almost the same computations, except that in the moment matching step it collects all the intermediate approximations $f_i(\bm{\theta}) \leftarrow \mathtt{proj}[\tilde{p}_i(\bm{\theta})] / q_{-1}(\bm{\theta})$ and use $\bar{f}(\bm{\theta})$ in the next inclusion step. 



%%%%
Unfortunately, finding out the loss function/free energy of AEP is not a trivial task. Instead we introduce a set of auxiliary distributions $\{q_i(\bm{\theta})\}$ and frame AEP as a double-loop algorithm. After the cavity computation step, the update $q(\bm{\theta})$ of AEP is constructed as follows:
%
\begin{itemize}
	\item \textbf{inner loop:} $q_i(\bm{\theta}) \leftarrow \arg\min_q KL(\tilde{p}_i(\bm{\theta}) ||q(\bm{\theta})) \buildrel\triangle\over = \mathtt{proj}[\tilde{p}_i(\bm{\theta})]$; 
	\item \textbf{outer loop:} $q(\bm{\theta}) \leftarrow \arg\min_q \sum_{i=1}^N KL(q(\bm{\theta}) ||q_i(\bm{\theta})) \buildrel\triangle\over = \mathtt{avg}[\{q_i(\bm{\theta})\}]$.
\end{itemize}
%
Note that the operator $\mathtt{avg}[\cdot]: \mathcal{Q}^N \rightarrow \mathcal{Q}$ can be defined on other distribution families similarly, and we abbreviate the formal definitions when re-introducing it later.
This framework allows us to analyse the convergence behaviour of AEP, where we have the following theorem.
%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
The fixed points of averaged EP, if exist, can be written as $q(\bm{\theta}) = \mathtt{avg}[\{q_i(\bm{\theta})\}]$, where
\begin{equation}
q_i(\bm{\theta}) = \mathtt{proj}[\tilde{p}_i(\bm{\theta})].
\label{eq:mm}
\end{equation}
These fixed points are also the fixed points of stochastic EP in expectation. 
\end{theorem}
\begin{proof}
In each update SEP gives the same answers as AEP in expectation if initialised at the same starting point. Also as an analogy to normal EP, the stationary points of AEP ask for moment matching between tilted and auxiliary distributions. 
\end{proof}
\textbf{Remark.} The convergence condition (\ref{eq:mm}) differs from the moment matching condition of full EP in that the matching requirements are proposed on the auxiliary distributions $\{q_i(\bm{\theta}) \}$ rather than the global approximation $q(\bm{\theta})$. The constraints are implicitly contained in the shared cavity distribution.

%
%
Averaged EP has also been proposed in \cite{barthelme:aep} but as a theoretical tool to study the convergence properties of normal EP. The authors showed that averaged EP can be interpreted as Newton methods in large data limit and proved the convergence chain EP $\rightarrow$ AEP $\rightarrow$ Newton in that case. However the results are very restrictive as it requires log-concave and very slow changing likelihood functions. 

\subsection{Connections to stochastic variational inference}
Applying stochastic optimisation to EP is also motivated by the great success of stochastic variational inference (SVI) \cite{hoffman:svi}. Both require memory allocation for global posterior only, compute updates based on the current sample, and perform updates with a suitable damping schedule. However SEP differs from SVI in the first view: the former processes local approximations, while the latter performs global optimisation. We attempt to connect SEP with SVI in a general framework extended from the double-loop algorithm, and we sketch two possible directions as local or global computation.

%
\textbf{Connecting SVI to a local optimisation algorithm.}
To understand SVI from a local optimisation perspective, consider VB as the expectation of SVI. We introduce a conceptual ``variational'' AEP by changing the moment matching step in the inner-loop to minimising the variational KL-divergence $KL(q_i(\bm{\theta}) || \tilde{p}_i(\bm{\theta}))$ wrt.~$q_i(\bm{\theta})$ \footnote{Also called information projection (I-projection) in \cite{amari:ig}).}. A similar version of EP is also presented in \cite{minka:divergence} as a special case of a generic message passing algorithm, where the author showed that EP with variational KL is equivalent to the variational message passing algorithm \cite{winn:vmp}. This result also applies to the ``variational'' AEP as the outer-loop computes a geometric average, with a formal proof in the supplementary material. Though the ``variational'' SEP has the same fixed points as SVI, it still benefits from the tractability of local approximation, especially when computing SVI requires a further level of approximation (e.g.~sampling or a second-level lower bound).

%
%%% 


%\subsection{Large-data limit revisited}
%We provide a comparison between SEP and SVI in large-data limit using the global optimisation framework. We denote $\alpha \mhyphen \mathtt{proj}[p(\bm{\theta})]$ as the $\alpha$-projection \cite{amari:ig1985} \cite{amari:ig} of any distribution $p(\bm{\theta})$ to the $\mathcal{Q}$ family. Using this notation the previously defined $\mathtt{proj}[\cdot]$ operator is the $1$-projection, while SVI takes $\alpha = \mhyphen 1$. A similar analysis as \cite{amari:alpha_proj} indicates that in the large-data limit ($N \rightarrow \infty$), the $\alpha$-projection step obtains a minimiser of the variational $KL(q(\bm{\theta}) || p(\bm{\theta} | \{\bm{x}_i\}^N))$. However it does not necessary imply the equivalence between SEP and SVI since both algorithms in the inner-loop do not directly minimise their divergence objective. Instead they iteratively compute the current stochastic estimate $q_i(\bm{\theta})$ based on the previous solution, e.g.~line 4 in SEP Algorithm \ref{alg:sep}, and more importantly that iterative process is not continuous wrt.~$\alpha$ at $\alpha = -1$. Aslo the inner-loop divergences are very unlikely to reach their minimum as the outer loop violates the gradient descent computations. Further, even all of the auxiliary distributions converges to the local optima of the variational KL-divergences, there is no guarantee that the global approximation after averaging converges to a VB local optimum. A better restatement of SEP/AEP with infinite observations would be that SEP tends to behave like SVI when $N$ goes to infinity, although the connection of fixed point behaviour is still unclear. 

%
\begin{figure}
\centering
\def\svgwidth{0.3\linewidth}
\subfigure[\label{fig:aep_vs_pep}]{
\input{fig/aep_vs_pep.pdf_tex}}
%
\hspace{0.5in}
%
\def\svgwidth{0.35\linewidth}
\subfigure[\label{fig:dep_sep_dsep}]{
\input{fig/dep_sep_dsep.pdf_tex}}

\caption{(a) A geometric view of AEP/PEP comparison. (b) A cartoon illustration of DEP, SEP and DSEP. For each algorithm we show $q(\bm{\theta})$ on the top and $\tilde{p}_i(\bm{\theta})$ at the bottom.}

\end{figure}