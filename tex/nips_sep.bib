@article{barthelme:aep,
author = {Dehaene, Guillaume and Barthelm\'e, Simon},
title = {Expectation propagation in the large-data limit},
journal = {arXiv:1503.08060},
year = {2015},
}

@incollection{turner+sahani:2011c,
 title ={Probabilistic amplitude and frequency demodulation},
 author={Richard E. Turner and Maneesh Sahani},
 booktitle = {Advances in Neural Information Processing Systems 24},
 editor = {J. Shawe-Taylor and R.S. Zemel and P. Bartlett and F.C.N. Pereira and K.Q. Weinberger},
 pages = {981--989},
 year = {2011},
abstract = {A number of recent scientific and engineering problems require
signals to be decomposed into a product of a slowly varying positive
envelope and a quickly varying carrier whose instantaneous frequency
also varies slowly over time. Although signal processing provides
algorithms for so-called amplitude- and frequency-demodulation (AFD),
there are well known problems with all of the existing
methods. Motivated by the fact that AFD is ill-posed, we approach the
problem using probabilistic inference. The new approach, called
probabilistic amplitude and frequency demodulation (PAFD), models
instantaneous frequency using an auto-regressive generalization of the
von Mises distribution, and the envelopes using Gaussian
auto-regressive dynamics with a positivity constraint. A novel form of
expectation propagation is used for inference. We demonstrate that
although PAFD is computationally demanding, it outperforms previous
approaches on synthetic and real signals in clean, noisy and missing
data settings.}
}

@article{gelman:dep,
author = {Andrew Gelman and Aki Vehtari and Pasi Jylänki and Christian Robert and Nicolas Chopin and John P. Cunningham},
title = {Expectation propagation as a way of life},
journal = {arXiv:1412.4869},
year = {2014},
}

@InCollection{turner+sahani:2011a,
  author = 	 {R. E. Turner and M. Sahani},
  title = 	 {Two problems with variational expectation
                  maximisation for time-series models},
  booktitle = 	 {Bayesian Time series models},
  pages = 	 {109--130},
  publisher = {Cambridge University Press},
  year = 	 2011,
  editor = 	 {D. Barber and T. Cemgil and S. Chiappa},
  chapter = 	 5,
  abstract ={Variational methods are a key component of the
                  approximate inference and learning toolbox. These
                  methods fill an important middle ground, retaining
                  distributional information about uncertainty in
                  latent variables, unlike maximum a posteriori
                  methods (MAP), and yet generally requiring less
                  computational time than Monte Carlo Markov Chain
                  methods. In particular the variational Expectation
                  Maximisation (vEM) and variational Bayes algorithms,
                  both involving variational optimisation of a
                  free-energy, are widely used in time-series
                  modelling. Here, we investigate the success of vEM
                  in simple probabilistic time-series models. First we
                  consider the inference step of vEM, and show that a
                  consequence of the well-known compactness property
                  of variational inference is a failure to propagate
                  uncertainty in time, thus limiting the usefulness of
                  the retained distributional information. In
                  particular, the uncertainty may appear to be
                  smallest precisely when the approximation is
                  poorest. Second, we consider parameter learning and
                  analytically reveal systematic biases in the
                  parameters found by vEM. Surprisingly, simpler
                  variational approximations (such a mean-field) can
                  lead to less bias than more complicated structured
                  approximations.  }
}

@inproceedings{broderick:stream,
  author = {Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C. and Jordan, Michael I.},
  booktitle = {NIPS},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  pages = {1727-1735},
  title = {Streaming Variational Bayes.},
  year = 2013
}

@inproceedings{xu:sms,
  author = {Minjie Xu and Balaji Lakshminarayanan and Yee Whye Teh and Jun Zhu and Bo Zhang},
  booktitle = {NIPS},
  title = {Distributed Bayesian Posterior Sampling via Moment Sharing.},
  year = 2014
}

@conference{minka:ep,
  author = {Minka, T.P.},
  booktitle = {Uncertainty in Artificial Intelligence},
  pages = {362--369},
  title = {{Expectation propagation for approximate Bayesian inference}},
  volume = 17,
  year = 2001
}

@techreport{minka:powerep,
  author = {Minka, T.P.},
  institution = {Microsoft Research, Cambridge},
  number = {MSR-TR-2004-149},
  title = {{Power EP}},
  year = 2004
}

@article{hoffman:svi,
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John William},
  journal = {Journal of Machine Learning Research},
  number = 1,
  pages = {1303-1347},
  title = {Stochastic variational inference.},
  volume = 14,
  year = 2013
}

@techreport{minka:divergence,
  author = {Minka, Thomas},
  institution = {Microsoft Research, Cambridge},
  number = {MSR-TR-2005-173},
  title = {Divergence measures and message passing},
  year = 2005
}

@article{miguel:pbp,
author = {Hernández-Lobato, J. M. and Adams, Ryan P.},
title = {Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks},
journal = {arXiv:1502.05336},
year = {2015},
}

@book{amari:ig,
  author = {Amari, Shun-ichi and Nagaoka, Hiroshi},
  publisher = {Oxford University Press},
  title = {Methods of information geometry},
  volume = 191,
  year = 2000
}

@inproceedings{qi:rep,
  author = {Qi, Yuan and Guo, Yandong},
  booktitle = {ICML (3)},
  pages = {262-270},
  publisher = {JMLR.org},
  series = {JMLR Proceedings},
  title = {Message passing with l1 penalized KL minimization.},
  volume = 28,
  year = 2013
}

@book{maybeck:adf,
  author = {Peter S. Maybeck},
  publisher = {Academic Press},
  title = {Stochastic models, estimation and control},
  year = 1982
}

@book{amari:ig1985,
  title = {Differential-Geometrical Methods in Statistic},
  publisher = {Springer},
  year = {1985},
  author = {Amari, Shun-ichi}
}

@article{amari:alpha_proj,
  title={Information geometry of $\alpha$-projection in mean field approximation},
  author={Amari, Shun-ichi and Ikeda, Shiro and Shimokawa, Hidetoshi},
  journal={Advanced Mean Field Methods},
  pages={241--257},
  year={2001},
  publisher={Cambridge, Massachusetts), The MIT Press}
}

@phdthesis{beal:variational,
  title={Variational algorithms for approximate Bayesian inference},
  author={Beal, Matthew James},
  year={2003},
  school={University of London}
}

@article{jordan:variational,
  title={An introduction to variational methods for graphical models},
  author={Jordan, Michael I and Ghahramani, Zoubin and Jaakkola, Tommi S and Saul, Lawrence K},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

@inproceedings{herbrich:trueskill,
  title={Trueskill™: A Bayesian skill rating system},
  author={Herbrich, Ralf and Minka, Tom and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={569--576},
  year={2006}
}

@article{opper:ec,
  title={Expectation consistent approximate inference},
  author={Opper, Manfred and Winther, Ole},
  journal={The Journal of Machine Learning Research},
  volume={6},
  pages={2177--2204},
  year={2005}
}

@article{kuss:gpep,
  title={Assessing approximate inference for binary Gaussian process classification},
  author={Kuss, Malte and Rasmussen, Carl Edward},
  journal={The Journal of Machine Learning Research},
  volume={6},
  pages={1679--1704},
  year={2005}
}

@article{barthelme:ep_likelihood,
  title={Expectation propagation for likelihood-free inference},
  author={Barthelm{\'e}, Simon and Chopin, Nicolas},
  journal={Journal of the American Statistical Association},
  volume={109},
  number={505},
  pages={315--333},
  year={2014}
}

@inproceedings{winn:vmp,
  title={Variational message passing},
  author={Winn, John M and Bishop, Christopher M},
  booktitle={Journal of Machine Learning Research},
  pages={661--694},
  year={2005}
}

@article{hoffman:nuts,
  title={The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014},
}

@misc{stan:pystan,
  author = {{Stan Development Team}},
  year = {2014},
  title = {PyStan: the Python interface to Stan,
           Version 2.5.0},
  url = {http://mc-stan.org/pystan.html}
}

@article{hernandez2015probabilistic,
  title={Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks},
  author={Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and Adams, Ryan P},
  journal={arXiv preprint arXiv:1502.05336},
  year={2015}
}

@inproceedings{mandt:smoothedSVI,
  title={Smoothed gradients for stochastic variational inference},
  author={Mandt, Stephan and Blei, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2438--2446},
  year={2014}
}

@inproceedings{ahn:distributedMCMC,
  title={Distributed stochastic gradient MCMC},
  author={Ahn, Sungjin and Shahbaba, Babak and Welling, Max},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={1044--1052},
  year={2014}
}

@inproceedings{bardenet:MCMC,
  title={Towards scaling up Markov chain Monte Carlo: an adaptive subsampling approach},
  author={Bardenet, R{\'e}mi and Doucet, Arnaud and Holmes, Chris},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={405--413},
  year={2014}
}

@article{cunningham:gaussianEP,
  title={Gaussian probabilities and expectation propagation},
  author={Cunningham, John P and Hennig, Philipp and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1111.6832},
  year={2011}
}

@inproceedings{qi+minka:sparseGP,
  author = {Qi, Yuan and Abdel-Gawad, Ahmed H and Minka, Thomas P},
  booktitle = { Uncertainty and Artificial Intelligence (UAI)},
  title = {Sparse-posterior Gaussian Processes for general likelihoods.},
  year = 2010
}

@article{blei:lda,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={the Journal of machine Learning research},
  volume={3},
  pages={993--1022},
  year={2003},
  publisher={JMLR. org}
}

@inproceedings{heess:learning_messages,
  title={Learning to pass expectation propagation messages},
  author={Heess, Nicolas and Tarlow, Daniel and Winn, John},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3219--3227},
  year={2013}
}

@inproceedings{jitkrittum:kernel,
  title={Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages},
  author={Jitkrittum, Wittawat and Gretton, Arthur and Heess, Nicolas and Eslami, SM and Lakshminarayanan, Balaji and Sejdinovic, Dino and Szab{\'o}, Zolt{\'a}n},
  booktitle={ Uncertainty in Artificial Intelligence },
  year={2015}
}