\subsection{Probabilistic back-propagation}

The final set of tests consider more complicated models and large datasets. Specifically we evaluate the methods for probabilistic-backpropagation (PBP) \cite{hernandez2015probabilistic}, a
recent state-of-the-art method for scalable approximate inference in neural
network models. Previous implementations of PBP perform several iterations of ADF over the training
data.  The moment-matching operations required by ADF are computed by
first propagating the uncertainty on the synaptic weights forward through the network
and then computing the gradient of the marginal likelihood by backpropagation.
PBP is based on ADF to reduce the large memory cost that would be required by EP
when the amount of available data is very large.

We performed several experiments to asses the accuracy of different implementations of PBP
based on ADF, SEP and EP. We considered neural networks with 100 hidden units and followed the same experimental protocol 
as described by \cite{miguel:pbp}%\todo[fancyline]{need to reference the datasets}.
Table 2 shows the average test RMSE and test log-likelihood for each method.
Interestingly, SEP and ADF often outperform EP in this case (although the results presented for ADF use a near-optimal number of sweeps and further iterations generally degrades performance). Finally, Table 3 shows the reduction in memory consumption when SEP is compared with EP.
This reduction scales as a function of the dataset size and for the biggest
one, Year, it is of several tens of gigabytes.
 
\begin{table} 
\small
\centering \label{tab:results} \begin{tabular}{l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}l@{\ica}r@{$\pm$}}\hline 
{} & \multicolumn{6}{c}{RMSE} & \multicolumn{6}{c}{test log-likelihood} \\
\bf{Dataset}&\multicolumn{2}{c}{\bf{ ADF }}&\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ EP }} &\multicolumn{2}{c}{\bf{ ADF }}&\multicolumn{2}{c}{\bf{ SEP }}&\multicolumn{2}{c}{\bf{ EP }} \\ \hline 
%
Kin8nm&0.098&0.0007&\bf{0.088}&\bf{0.0009}&0.089&0.0006
	&0.896&0.006&\bf{1.013}&\bf{0.011}&1.005&0.007\\ 
%
Naval&0.006&0.0000&\bf{0.002}&\bf{0.0000}&0.004&0.0000
	&3.731&0.006&\bf{4.590}&\bf{0.014}&4.207&0.011\\  
%
Power&\bf{4.124}&\bf{0.0345}&4.165&0.0336&4.191&0.0349
	&\bf{-2.837}&\bf{0.009}&-2.846&0.008&-2.852&0.008\\
% 
Protein&4.727&0.0112&\bf{4.670}&\bf{0.0109}&4.748&0.0137
	&-2.973&0.003&\bf{-2.961}&\bf{0.003}&-2.979&0.003\\ 
%
Wine&\bf{0.635}&\bf{0.0079}&0.650&0.0082&0.637&0.0076
	&-0.968&0.014&-0.976&0.013&\bf{-0.958}&\bf{0.011}\\  
%
Year&\bf{8.879}&\bf{   NA}&8.922&   NA&8.914&   NA
&\bf{-3.603}&\bf{  NA}&-3.924&  NA&-3.929&  NA\\
 \hline \end{tabular} 
\caption{Average test results for all methods. Datasets are also from the UCI machine learning repository.} 
 \end{table} 


%\begin{table} 
%\begin{minipage}[!t]{0.6\linewidth}
%\small
%\centering 
%\begin{tabular}{lrrr}\hline \bf{Dataset}& $N$ & $d$ & MB reduction\\ \hline 
%Kin8nm & 8,192 & 8 & 58MB \\ 
%Naval & 11,934 & 16 & 147MB\\ 
%Power Plant & 9,568 & 4 & 37MB\\ 
%Protein & 45,730 & 9 & \bf{694MB}\\ 
%Wine & 1,599 & 11 & 14MB\\ 
%Year & 515,340 & 90 & \bf{65107MB}\\ \hline 
%\end{tabular} 
%\caption{ Memory reduction of SEP from full EP. } \label{tab:memory} 
%\end{minipage}
%\end{table} 
%
%\begin{minipage}[!t]{0.35\linewidth}
%\begin{figure}

%\centering
%\def\svgwidth{0.9\linewidth}
%\subfigure[\label{fig:sonar}]{
%\input{fig/sonar.pdf_tex}}
%\caption{Averaged test likelihood for long-time training.}

%\end{figure}
%\end{minipage}
