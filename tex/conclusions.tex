\section{Conclusions and future work}
This paper has presented the stochastic expectation propagation method for reducing EP's large memory consumption that is prohibitive for large datasets. We have connected the new algorithm to a number of existing methods including assumed density filtering, variational message passing, variational inference, stochastic variational inference and averaged EP.
%
Experiments on Bayesian logistic regression (both synthetic and real world) and Mixture of Gaussians clustering indicated that the new method had an accuracy that was competitive with EP.  Experiments on the probabilistic back-propagation on large real world regression datasets again showed that SEP comparably to EP with a vastly reduced memory footprint. 
%
Future experimental work will focus on developing data-partitioning methods to leverage finer-grained approximations (DESP) that showed promising experimental performance and also mini-batch updates. Theoretical work will study the convergence properties of the new algorithms for which we only have limited results at present.